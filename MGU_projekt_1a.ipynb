{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MGU_projekt_1a.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKHlueYTMrFb",
        "colab_type": "text"
      },
      "source": [
        "# Sieć neuronowa\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "635Py62nMhC0",
        "colab_type": "text"
      },
      "source": [
        "## Funkcje aktywacji"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZW2F_-9Mck-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1.0 - sigmoid(x))\n",
        "\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(x, 0)\n",
        "\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1.0, 0.0)\n",
        "\n",
        "\n",
        "def identity(x):\n",
        "    return x\n",
        "\n",
        "\n",
        "def identity_derivative(x):\n",
        "    return np.array([[1] for i in x])\n",
        "\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "\n",
        "def tanh_derivative(x):\n",
        "    return 1.0 - np.square(np.tanh(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ul6Q6T2F4Y-d",
        "colab_type": "text"
      },
      "source": [
        "## Pochodne funkcji straty"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2j29Du94e1F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def quadratic_cost_derivative(output_activations, y):\n",
        "    return (output_activations - y)\n",
        "\n",
        "def cross_entropy_cost_derivative(output_activations, y):\n",
        "    return (output_activations - y)/((1-output_activations) * output_activations)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXzf8CRcNL8q",
        "colab_type": "text"
      },
      "source": [
        "## Klasa sieci"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gb4EE9VbNSMS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import codecs\n",
        "from enum import Enum\n",
        "\n",
        "class TaskType(Enum):\n",
        "    Classification = 1\n",
        "    Regression = 2\n",
        "\n",
        "class NeuralNetwork:\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      layer_sizes,\n",
        "      activation_function,\n",
        "      activation_function_deriv,\n",
        "      output_function,\n",
        "      output_function_deriv,\n",
        "      cost_function_deriv,\n",
        "      using_bias,\n",
        "      taskType,\n",
        "      seed = None\n",
        "    ):\n",
        "    self.layer_num = len(layer_sizes)\n",
        "    self.layer_sizes = layer_sizes\n",
        "\n",
        "    if(seed != None):\n",
        "      np.random.seed(seed)\n",
        "\n",
        "    self.edges_weights = [np.random.randn(x, y) for x, y in zip(layer_sizes[1:], layer_sizes[:-1])]\n",
        "    self.biases_weights = [np.random.rand(x, 1) for x in layer_sizes[1:]]\n",
        "    self.activation_function = activation_function\n",
        "    self.activation_function_deriv = activation_function_deriv\n",
        "    self.output_function = output_function\n",
        "    self.output_function_deriv = output_function_deriv\n",
        "    self.cost_function_deriv = cost_function_deriv\n",
        "    self.using_bias = using_bias\n",
        "    self.weighted_sums = []\n",
        "    self.activations = []\n",
        "    self.last_delta_weight = None\n",
        "    self.last_delta_biases = None\n",
        "    self.moment_factor = None\n",
        "    self.learning_factor = None\n",
        "    self.batch_size = None\n",
        "    self.epoch_num = None\n",
        "    self.history = []\n",
        "    self.taskType = taskType\n",
        "    self.classification_area_probing = []\n",
        "\n",
        "  def backpropagate(self, net_input, desired_output):\n",
        "    delta_weight = [np.zeros(es.shape) for es in self.edges_weights]\n",
        "    delta_biases = [np.zeros(bs.shape) for bs in self.biases_weights]\n",
        "\n",
        "    self.activations = []\n",
        "    self.weighted_sums = []\n",
        "\n",
        "    if(self.using_bias):\n",
        "      net_output = self.feed_forward(net_input)\n",
        "    else:\n",
        "      net_output = self.feed_forward_no_bias(net_input)\n",
        "\n",
        "    cost_per_node = self.cost_function_deriv(net_output, desired_output)\n",
        "    ofd = self.output_function_deriv(self.weighted_sums[-1])\n",
        "    d_per_node = cost_per_node * ofd\n",
        "    \n",
        "    delta_weight[-1] = np.dot(d_per_node, self.activations[-2].transpose())\n",
        "    if(self.using_bias):\n",
        "      delta_biases[-1] = d_per_node;\n",
        "\n",
        "    for l in range(2, self.layer_num):\n",
        "      ws = self.weighted_sums[-l]\n",
        "      afd = self.activation_function_deriv(ws)\n",
        "      d_per_node = np.dot(self.edges_weights[-l+1].transpose(), d_per_node) * afd\n",
        "      \n",
        "      delta_weight[-l] = np.dot(d_per_node, self.activations[-l-1].transpose())\n",
        "      \n",
        "      if(self.using_bias):\n",
        "        delta_biases[-l] = d_per_node\n",
        "      \n",
        "\n",
        "    return (delta_weight, delta_biases)\n",
        "\n",
        "  def feed_forward(self, net_input):\n",
        "    tmp_output = net_input\n",
        "    self.activations.append(net_input)\n",
        "\n",
        "    for bias, layer_weights in zip(self.biases_weights[:-1], self.edges_weights[:-1]):\n",
        "      layer_ws = np.dot(layer_weights, tmp_output) + bias\n",
        "      self.weighted_sums.append(layer_ws)\n",
        "      layer_activs = self.activation_function(layer_ws)\n",
        "      self.activations.append(layer_activs)\n",
        "      tmp_output = layer_activs\n",
        "\n",
        "    layer_ws = np.dot(self.edges_weights[-1], tmp_output) + self.biases_weights[-1]\n",
        "    self.weighted_sums.append(layer_ws)\n",
        "    layer_activs = self.output_function(layer_ws)\n",
        "    self.activations.append(layer_activs)\n",
        "    return layer_activs\n",
        "\n",
        "  def feed_forward_no_bias(self, net_input):\n",
        "    tmp_output = net_input\n",
        "    self.activations.append(net_input)\n",
        "\n",
        "    for layer_weights in self.edges_weights[:-1]:\n",
        "      layer_ws = np.dot(layer_weights, tmp_output)\n",
        "      self.weighted_sums.append(layer_ws)\n",
        "      layer_activs = self.activation_function(layer_ws)\n",
        "      self.activations.append(layer_activs)\n",
        "      tmp_output = layer_activs\n",
        "\n",
        "    layer_ws = np.dot(self.edges_weights[-1], tmp_output)\n",
        "    self.weighted_sums.append(layer_ws)\n",
        "    layer_activs = self.output_function(layer_ws)\n",
        "    self.activations.append(layer_activs)\n",
        "    return layer_activs\n",
        "\n",
        "  def learn(self, training_set, test_set, learning_factor, moment_factor, batch_size, epoch_num, probe=None):\n",
        "    training_set_cp = training_set\n",
        "\n",
        "    self.moment_factor = moment_factor\n",
        "    self.learning_factor = learning_factor\n",
        "    self.last_delta_biases = None\n",
        "    self.last_delta_weight = None\n",
        "    self.batch_size = batch_size\n",
        "    self.epoch_num = epoch_num\n",
        "\n",
        "    \n",
        "    for i in range(epoch_num):\n",
        "      np.random.shuffle(training_set_cp)\n",
        "      batches = [training_set_cp[i * batch_size:(i + 1) * batch_size] \n",
        "               for i in range((len(training_set_cp) + batch_size - 1) // batch_size )]\n",
        "      \n",
        "      for batch, batch_id in zip(batches, range(1, 1 + len(batches))):\n",
        "        sum_dw = [np.zeros(ew.shape) for ew in self.edges_weights]\n",
        "        sum_db = [np.zeros(bs.shape) for bs in self.biases_weights]\n",
        "        for ni, dno in batch:\n",
        "\n",
        "          dw, db = self.backpropagate(ni, dno)\n",
        "          sum_dw = [sdw + ndw for sdw, ndw in zip(sum_dw, dw)]\n",
        "          if(self.using_bias):\n",
        "            sum_db = [sdb + ndb for sdb, ndb in zip(sum_db, db)]\n",
        "        \n",
        "        if(self.last_delta_weight != None):\n",
        "          sum_dw = [self.moment_factor * ldw + (1 - self.moment_factor) * sdw \n",
        "                    for sdw, ldw in zip(sum_dw, self.last_delta_weight)]\n",
        "          if(self.using_bias):\n",
        "            sum_db = [self.moment_factor * ldb + (1 - self.moment_factor) * sdb \n",
        "                      for sdb, ldb in zip(sum_db, self.last_delta_biases)]\n",
        "\n",
        "        self.edges_weights = [ew - (learning_factor / len(batch)) * sdw for ew, sdw in zip(self.edges_weights, sum_dw)]\n",
        "        \n",
        "        if(self.using_bias):\n",
        "          self.biases_weights = [bw - (learning_factor / len(batch)) * sdb for bw, sdb in zip(self.biases_weights, sum_db)]\n",
        "\n",
        "        self.last_delta_biases = sum_db\n",
        "        self.last_delta_weight = sum_dw\n",
        "\n",
        "        test_acc = self.test_accuracy(test_set)\n",
        "        train_acc = self.test_accuracy(training_set)\n",
        "\n",
        "        if(self.taskType == TaskType.Classification):\n",
        "          print(f\"Epoch {i}: [{batch_id}/{len(batches)}] - Accuracy: {train_acc} / {len(training_set)}\")\n",
        "        else:\n",
        "          print(f\"Epoch {i}: [{batch_id}/{len(batches)}] - Accuracy: {test_acc}\")\n",
        "\n",
        "        self.loss = 0\n",
        "\n",
        "        self.take_snapshot(\n",
        "          f'Epoch {i}: [{batch_id}/{len(batches)}]', train_acc, test_acc)\n",
        "        if probe != None:\n",
        "          self.classification_area_probing.append(self.classify(probe))\n",
        "        \n",
        "  def test_accuracy(self, test_data):\n",
        "    if(self.taskType == TaskType.Classification):\n",
        "      return self.validate_classification(test_data)\n",
        "    else:\n",
        "      return self.validate_regression(test_data)\n",
        "  \n",
        "  def validate_classification(self, data):\n",
        "      correct = 0\n",
        "      for x, y in data:\n",
        "          expected = np.argmax(y)\n",
        "          result = np.argmax(self.feed_forward(x))\n",
        "          if expected == result:\n",
        "              correct = correct + 1\n",
        "      return correct\n",
        "  \n",
        "  def validate_regression(self, data):\n",
        "    error = 0\n",
        "    for x, y in data:\n",
        "        error = error + np.square(self.feed_forward(x)-y).mean()\n",
        "    return error / len(data)\n",
        "\n",
        "  def classify(self, test_data):\n",
        "        results = []\n",
        "        for x in test_data:\n",
        "            classification = np.argmax(self.feed_forward(x))\n",
        "            results.append(classification)\n",
        "        return results\n",
        "      \n",
        "  def take_snapshot(self, name, train_accuracy=None, test_accuracy=None):\n",
        "    if(self.using_bias):\n",
        "      dump = dict(biases=[bias.tolist() for bias in self.biases_weights],\n",
        "                  weight_matrix=[weight.tolist() for weight in self.edges_weights],\n",
        "                  name=name,\n",
        "                  train_fitness=train_accuracy,\n",
        "                  test_fitness=test_accuracy)\n",
        "    else:\n",
        "      dump = dict(weight_matrix=[weight.tolist() for weight in self.edges_weights],\n",
        "                  name=name,\n",
        "                  train_fitness=train_accuracy,\n",
        "                  test_fitness=test_accuracy)\n",
        "      \n",
        "    self.history.append(dump)\n",
        "\n",
        "  def save_history(self, out_filepath):\n",
        "    network_info = dict(\n",
        "        sizes=self.layer_sizes, learning_factor=self.learning_factor,\n",
        "        moment_factor=self.moment_factor,\n",
        "        epoch_num=self.epoch_num, batch_size=self.batch_size,\n",
        "        activation_function=self.activation_function.__name__,\n",
        "        output_function=self.output_function.__name__)\n",
        "    json.dump(dict(history=self.history, network_info=network_info),\n",
        "              codecs.open(out_filepath, 'w', encoding='utf-8'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3g1s3G3468Ga",
        "colab_type": "text"
      },
      "source": [
        "## Podpięcie dysku i wczytywanie danych"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9kob-tk7C5h",
        "colab_type": "text"
      },
      "source": [
        "### Podpięcie dysku"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlPVGpq77Ewk",
        "colab_type": "code",
        "outputId": "4eeca1b5-164b-4baf-d859-8137c4f7d8a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "BASE_DIR = '/content/gdrive/My Drive/DL2020/Projekt1a'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-naFIiYM7Gi8",
        "colab_type": "text"
      },
      "source": [
        "### Ładowanie danych"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awnuWVRX7Ih7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def load_classification_wrapper(path, name, train_size, test_size=None):\n",
        "    if test_size == None:\n",
        "        test_size = train_size\n",
        "\n",
        "    train = load_classification(os.path.join(\n",
        "        path, f'{name}.train.{train_size}.csv'), ['x', 'y'])\n",
        "    test = load_classification(os.path.join(\n",
        "        path, f'{name}.test.{test_size}.csv'), ['x', 'y'])\n",
        "    return (train, test)\n",
        "\n",
        "\n",
        "def load_classification(csv_filename, argument_column_names, class_column_name='cls'):\n",
        "    df = pd.read_csv(csv_filename)\n",
        "\n",
        "    output_layer_neuron_count = len(df[class_column_name].unique())\n",
        "    input_layer_neuron_count = len(argument_column_names)\n",
        "\n",
        "    training_data = []\n",
        "\n",
        "    for row in df.itertuples():\n",
        "        x = np.zeros((input_layer_neuron_count, 1))\n",
        "\n",
        "        for i in range(len(argument_column_names)):\n",
        "            x[i] = getattr(row, argument_column_names[i])\n",
        "\n",
        "        y = vectorize_class(getattr(row, class_column_name),\n",
        "                            output_layer_neuron_count)\n",
        "\n",
        "        training_data.append((x, y))\n",
        "    return training_data\n",
        "\n",
        "def load_regression(csv_filename, input_column_names, output_column_names):\n",
        "    df = pd.read_csv(csv_filename)\n",
        "\n",
        "    output_layer_neuron_count = len(output_column_names)\n",
        "    input_layer_neuron_count = len(input_column_names)\n",
        "\n",
        "    training_data = []\n",
        "\n",
        "    for row in df.itertuples():\n",
        "        x = np.zeros((input_layer_neuron_count, 1))\n",
        "        y = np.zeros((output_layer_neuron_count, 1))\n",
        "\n",
        "        for i in range(len(input_column_names)):\n",
        "            x[i] = getattr(row, input_column_names[i])\n",
        "\n",
        "        for i in range(len(output_column_names)):\n",
        "            y[i] = getattr(row, output_column_names[i])\n",
        "\n",
        "        training_data.append((x, y))\n",
        "    return training_data\n",
        "\n",
        "def load_regression_wrapper(path, name, train_size, test_size=None):\n",
        "    if test_size == None:\n",
        "        test_size = train_size\n",
        "\n",
        "    train = load_regression(os.path.join(\n",
        "        path, f'{name}.train.{train_size}.csv'), ['x'], ['y'])\n",
        "    test = load_regression(os.path.join(\n",
        "        path, f'{name}.test.{test_size}.csv'), ['x'], ['y'])\n",
        "    return (train, test)\n",
        "\n",
        "\n",
        "def vectorize_class(class_id, class_length):\n",
        "    # assume that classification starts at 1 to class_length, e.g. 1,2,3,4\n",
        "    class_id = class_id - 1\n",
        "    y = np.zeros((class_length, 1))\n",
        "    y[class_id] = 1\n",
        "    return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80unqXUN7-vW",
        "colab_type": "text"
      },
      "source": [
        "## Odpalenie sieci"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vq0gcLoPrT0q",
        "colab_type": "text"
      },
      "source": [
        "### Klasyfikacja"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnNerGan8BYI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "train, test = load_classification_wrapper(BASE_DIR + r'/data/classification', 'data.three_gauss', 100)\n",
        "inp_lay_size = len(train[0][0])\n",
        "out_lay_size = len(train[0][1])\n",
        "np.random.seed(11)\n",
        "nn = NeuralNetwork([inp_lay_size, 8, out_lay_size], sigmoid, sigmoid_derivative, sigmoid, sigmoid_derivative, quadratic_cost_derivative, True, TaskType.Classification)\n",
        "nn.learn(train, test, 0.9, 0.3, 20, 500)\n",
        "\n",
        "now = datetime.now()\n",
        "current_time = now.strftime(\"%H:%M:%S\")\n",
        "\n",
        "nn.save_history(BASE_DIR + f'/results/classification_{current_time}.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhSifZGurZo_",
        "colab_type": "text"
      },
      "source": [
        "### Regresja"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOmG42C3c3QK",
        "colab_type": "text"
      },
      "source": [
        "## Testy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zISQ5_Myc6pJ",
        "colab_type": "text"
      },
      "source": [
        "### Feed forward"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE5bhR9PdANF",
        "colab_type": "code",
        "outputId": "02874af5-118b-422d-f5a7-815d2ab6547d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "np.random.seed(0)\n",
        "l_sizes = [2,2,2]\n",
        "w = [np.random.randn(x, y) * 0.01 for x, y in zip(l_sizes[1:], l_sizes[:-1])]\n",
        "b = [np.random.rand(x, 1) for x in l_sizes[1:]]\n",
        "\n",
        "inp = np.array([[1], [1]])\n",
        "res2 = inp\n",
        "for ww, bb in zip(w, b):\n",
        "  res2 = identity(np.dot(ww, res2) + bb)\n",
        "\n",
        "net = NeuralNetwork([2, 2, 2], identity, identity_derivative, identity, identity_derivative, quadratic_cost_derivative, True, TaskType.Classification, 0)\n",
        "res = net.feed_forward(inp)\n",
        "print(res)\n",
        "print(res2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.80606424]\n",
            " [0.53762709]]\n",
            "[[0.80606424]\n",
            " [0.53762709]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jG41OXvmV7Q",
        "colab_type": "text"
      },
      "source": [
        "### Błąd = 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyRgIUphmUC4",
        "colab_type": "code",
        "outputId": "1bc0f200-3c0f-4f89-c268-34c5327339dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "np.random.seed(0)\n",
        "l_sizes = [2,2,2]\n",
        "output = np.array([[0.80606424], [0.53762709]])\n",
        "inp = np.array([[1], [1]])\n",
        "\n",
        "net = NeuralNetwork([2, 2, 2], identity, identity_derivative, identity, identity_derivative, quadratic_cost_derivative, True, TaskType.Classification, 0)\n",
        "w, b = net.backpropagate(inp, output)\n",
        "print(w)\n",
        "print(b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[ 1.28539538e-11,  1.28539538e-11],\n",
            "       [-9.31866041e-12, -9.31866041e-12]]), array([[ 1.05391157e-09,  4.44578668e-10],\n",
            "       [-7.38599336e-10, -3.11568368e-10]])]\n",
            "[array([[ 1.28539538e-11],\n",
            "       [-9.31866041e-12]]), array([[ 1.06962994e-09],\n",
            "       [-7.49615037e-10]])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5AAfuVYwtc2",
        "colab_type": "text"
      },
      "source": [
        "### Uczenie nauczonej sieci"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Krur2o9Yw3xF",
        "colab_type": "code",
        "outputId": "d4cb1b74-88fd-4dac-e283-31ab28bcd336",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "np.random.seed(0)\n",
        "l_sizes = [2,2,2]\n",
        "w = [np.random.randn(x, y) * 0.01 for x, y in zip(l_sizes[1:], l_sizes[:-1])]\n",
        "b = [np.random.rand(x, 1) for x in l_sizes[1:]]\n",
        "\n",
        "training_set = [(np.array([[1], [1]]), np.array([[0.80606424], [0.53762709]]))]\n",
        "test_set = [(np.array([[1], [1]]), np.array([[0.80606424], [0.53762709]]))]\n",
        "\n",
        "net = NeuralNetwork([2, 2, 2], identity, identity_derivative, identity, identity_derivative, quadratic_cost_derivative, True, TaskType.Classification, 0)\n",
        "net.learn(training_set, test_set, 0.1, 0.1, 1, 1)\n",
        "print(net.feed_forward(np.array([[1], [1]])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: [1/1] - Train set accuracy: 1 / 1\n",
            "Epoch 0: [1/1] - Test set accuracy: 1\n",
            "[[0.80606424]\n",
            " [0.53762709]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZVyAWbtbwTK",
        "colab_type": "text"
      },
      "source": [
        "## Wyniki"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AK8cQC16b7pV",
        "colab_type": "text"
      },
      "source": [
        "### Wyniki klasyfikacji"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alc9vAD0cW6B",
        "colab_type": "text"
      },
      "source": [
        "#### Wizualizacja zbioru uczącego oraz rezultatów klasyfikacji"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jj6pFqrBclBY",
        "colab_type": "code",
        "outputId": "66fa5f08-fc12-4bf7-ab24-b79f74e24a82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import matplotlib.pyplot as pyplot\n",
        "\n",
        "def create_filename(figure_name, data_name, epochs, batch_size):\n",
        "    return BASE_DIR + f'/results/classification/{figure_name}__{data_name.split(\".\")[1]}__{epochs}_{batch_size}.png'\n",
        "\n",
        "def create_heatmap(min_x, max_x, min_y, max_y, count_x, count_y):\n",
        "    step_x = (max_x - min_x) / count_x\n",
        "    step_y = (max_y - min_y) / count_y\n",
        "    probes_x = []\n",
        "    probes_y = []\n",
        "    probes_input = []\n",
        "    for x_i in range(count_x):\n",
        "        for y_i in range(count_y):\n",
        "            x = min_x + x_i * step_x\n",
        "            y = min_y + y_i * step_y\n",
        "            probes_x.append(x)\n",
        "            probes_y.append(y)\n",
        "            input_x = np.zeros((2, 1))\n",
        "            input_x[0] = x\n",
        "            input_x[1] = y\n",
        "            probes_input.append(input_x)\n",
        "    return probes_x, probes_y, probes_input\n",
        "\n",
        "def process(seed, hidden_layer_sizes, hidden_layers_activation_function, hidden_layers_activation_function_prime, output_layer_activation_function, output_layer_activation_function_prime, cost_derivative, is_bias_enabled, path, data_name, train_size,  epochs, batch_size, eta, alpha, test_size=None):\n",
        "    global nn\n",
        "    train_data, test_data = load_classification_wrapper(path, data_name, train_size, test_size)\n",
        "    np.random.seed(seed)\n",
        "    sizes = [len(train_data[0][0])]\n",
        "    sizes.extend(hidden_layer_sizes)\n",
        "    sizes.append(len(train_data[0][1]))\n",
        "    nn = NeuralNetwork(sizes, hidden_layers_activation_function, hidden_layers_activation_function_prime, output_layer_activation_function, output_layer_activation_function_prime, cost_derivative, is_bias_enabled, TaskType.Classification)\n",
        "\n",
        "    x = [entry[0][0][0] for entry in test_data]\n",
        "    y = [entry[0][1][0] for entry in test_data]\n",
        "\n",
        "    probes_x, probes_y, probes_input = create_heatmap(\n",
        "        min(x), max(x), min(y), max(y), 300, 300)\n",
        "\n",
        "    nn.learn(train_data, test_data, eta, alpha, batch_size, epochs)#, probe=probes_input)\n",
        "    \n",
        "    # classification points visualization\n",
        "    valid_classification = [np.argmax(entry[1]) for entry in test_data]\n",
        "    net_classification = nn.classify([x for (x, y) in test_data])\n",
        "    x = [entry[0][0][0] for entry in test_data]\n",
        "    y = [entry[0][1][0] for entry in test_data]\n",
        "\n",
        "    pyplot.scatter(x=x, y=y, c=net_classification)\n",
        "    pyplot.axis('equal')\n",
        "    pyplot.savefig(create_filename('net-classification', data_name, epochs, batch_size), dpi=150)\n",
        "    pyplot.show()\n",
        "    pyplot.clf()\n",
        "\n",
        "    # classificaiton area probing\n",
        "\n",
        "    for probe_classification, epoch in zip(nn.classification_area_probing, range(len(nn.classification_area_probing))):\n",
        "        pyplot.scatter(x=probes_x, y=probes_y, c=probe_classification)\n",
        "        pyplot.axis('equal')\n",
        "        pyplot.savefig(create_filename(f'probing-classification-{epoch}', data_name, epochs, batch_size), dpi=150)\n",
        "        pyplot.show()\n",
        "        pyplot.clf()\n",
        "\n",
        "process(0, [5], sigmoid, sigmoid_derivative, sigmoid, sigmoid_derivative, quadratic_cost_derivative, True, BASE_DIR + r'/data/classification', 'data.three_gauss', 100, 10, 10, 3, 0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: [1/30] - Accuracy: 100 / 300\n",
            "Epoch 0: [2/30] - Accuracy: 100 / 300\n",
            "Epoch 0: [3/30] - Accuracy: 100 / 300\n",
            "Epoch 0: [4/30] - Accuracy: 100 / 300\n",
            "Epoch 0: [5/30] - Accuracy: 100 / 300\n",
            "Epoch 0: [6/30] - Accuracy: 100 / 300\n",
            "Epoch 0: [7/30] - Accuracy: 100 / 300\n",
            "Epoch 0: [8/30] - Accuracy: 100 / 300\n",
            "Epoch 0: [9/30] - Accuracy: 100 / 300\n",
            "Epoch 0: [10/30] - Accuracy: 100 / 300\n",
            "Epoch 0: [11/30] - Accuracy: 100 / 300\n",
            "Epoch 0: [12/30] - Accuracy: 100 / 300\n",
            "Epoch 0: [13/30] - Accuracy: 100 / 300\n",
            "Epoch 0: [14/30] - Accuracy: 100 / 300\n",
            "Epoch 0: [15/30] - Accuracy: 100 / 300\n",
            "Epoch 0: [16/30] - Accuracy: 100 / 300\n",
            "Epoch 0: [17/30] - Accuracy: 100 / 300\n",
            "Epoch 0: [18/30] - Accuracy: 100 / 300\n",
            "Epoch 0: [19/30] - Accuracy: 100 / 300\n",
            "Epoch 0: [20/30] - Accuracy: 100 / 300\n",
            "Epoch 0: [21/30] - Accuracy: 100 / 300\n",
            "Epoch 0: [22/30] - Accuracy: 100 / 300\n",
            "Epoch 0: [23/30] - Accuracy: 100 / 300\n",
            "Epoch 0: [24/30] - Accuracy: 100 / 300\n",
            "Epoch 0: [25/30] - Accuracy: 100 / 300\n",
            "Epoch 0: [26/30] - Accuracy: 100 / 300\n",
            "Epoch 0: [27/30] - Accuracy: 100 / 300\n",
            "Epoch 0: [28/30] - Accuracy: 100 / 300\n",
            "Epoch 0: [29/30] - Accuracy: 100 / 300\n",
            "Epoch 0: [30/30] - Accuracy: 100 / 300\n",
            "Epoch 1: [1/30] - Accuracy: 100 / 300\n",
            "Epoch 1: [2/30] - Accuracy: 100 / 300\n",
            "Epoch 1: [3/30] - Accuracy: 100 / 300\n",
            "Epoch 1: [4/30] - Accuracy: 100 / 300\n",
            "Epoch 1: [5/30] - Accuracy: 100 / 300\n",
            "Epoch 1: [6/30] - Accuracy: 100 / 300\n",
            "Epoch 1: [7/30] - Accuracy: 100 / 300\n",
            "Epoch 1: [8/30] - Accuracy: 100 / 300\n",
            "Epoch 1: [9/30] - Accuracy: 100 / 300\n",
            "Epoch 1: [10/30] - Accuracy: 100 / 300\n",
            "Epoch 1: [11/30] - Accuracy: 100 / 300\n",
            "Epoch 1: [12/30] - Accuracy: 100 / 300\n",
            "Epoch 1: [13/30] - Accuracy: 100 / 300\n",
            "Epoch 1: [14/30] - Accuracy: 100 / 300\n",
            "Epoch 1: [15/30] - Accuracy: 100 / 300\n",
            "Epoch 1: [16/30] - Accuracy: 100 / 300\n",
            "Epoch 1: [17/30] - Accuracy: 100 / 300\n",
            "Epoch 1: [18/30] - Accuracy: 100 / 300\n",
            "Epoch 1: [19/30] - Accuracy: 100 / 300\n",
            "Epoch 1: [20/30] - Accuracy: 100 / 300\n",
            "Epoch 1: [21/30] - Accuracy: 100 / 300\n",
            "Epoch 1: [22/30] - Accuracy: 100 / 300\n",
            "Epoch 1: [23/30] - Accuracy: 100 / 300\n",
            "Epoch 1: [24/30] - Accuracy: 100 / 300\n",
            "Epoch 1: [25/30] - Accuracy: 100 / 300\n",
            "Epoch 1: [26/30] - Accuracy: 100 / 300\n",
            "Epoch 1: [27/30] - Accuracy: 100 / 300\n",
            "Epoch 1: [28/30] - Accuracy: 100 / 300\n",
            "Epoch 1: [29/30] - Accuracy: 100 / 300\n",
            "Epoch 1: [30/30] - Accuracy: 100 / 300\n",
            "Epoch 2: [1/30] - Accuracy: 100 / 300\n",
            "Epoch 2: [2/30] - Accuracy: 100 / 300\n",
            "Epoch 2: [3/30] - Accuracy: 100 / 300\n",
            "Epoch 2: [4/30] - Accuracy: 100 / 300\n",
            "Epoch 2: [5/30] - Accuracy: 100 / 300\n",
            "Epoch 2: [6/30] - Accuracy: 100 / 300\n",
            "Epoch 2: [7/30] - Accuracy: 100 / 300\n",
            "Epoch 2: [8/30] - Accuracy: 100 / 300\n",
            "Epoch 2: [9/30] - Accuracy: 100 / 300\n",
            "Epoch 2: [10/30] - Accuracy: 100 / 300\n",
            "Epoch 2: [11/30] - Accuracy: 100 / 300\n",
            "Epoch 2: [12/30] - Accuracy: 100 / 300\n",
            "Epoch 2: [13/30] - Accuracy: 100 / 300\n",
            "Epoch 2: [14/30] - Accuracy: 100 / 300\n",
            "Epoch 2: [15/30] - Accuracy: 100 / 300\n",
            "Epoch 2: [16/30] - Accuracy: 100 / 300\n",
            "Epoch 2: [17/30] - Accuracy: 100 / 300\n",
            "Epoch 2: [18/30] - Accuracy: 100 / 300\n",
            "Epoch 2: [19/30] - Accuracy: 100 / 300\n",
            "Epoch 2: [20/30] - Accuracy: 100 / 300\n",
            "Epoch 2: [21/30] - Accuracy: 100 / 300\n",
            "Epoch 2: [22/30] - Accuracy: 100 / 300\n",
            "Epoch 2: [23/30] - Accuracy: 100 / 300\n",
            "Epoch 2: [24/30] - Accuracy: 100 / 300\n",
            "Epoch 2: [25/30] - Accuracy: 100 / 300\n",
            "Epoch 2: [26/30] - Accuracy: 100 / 300\n",
            "Epoch 2: [27/30] - Accuracy: 100 / 300\n",
            "Epoch 2: [28/30] - Accuracy: 100 / 300\n",
            "Epoch 2: [29/30] - Accuracy: 182 / 300\n",
            "Epoch 2: [30/30] - Accuracy: 100 / 300\n",
            "Epoch 3: [1/30] - Accuracy: 100 / 300\n",
            "Epoch 3: [2/30] - Accuracy: 100 / 300\n",
            "Epoch 3: [3/30] - Accuracy: 100 / 300\n",
            "Epoch 3: [4/30] - Accuracy: 100 / 300\n",
            "Epoch 3: [5/30] - Accuracy: 100 / 300\n",
            "Epoch 3: [6/30] - Accuracy: 100 / 300\n",
            "Epoch 3: [7/30] - Accuracy: 100 / 300\n",
            "Epoch 3: [8/30] - Accuracy: 100 / 300\n",
            "Epoch 3: [9/30] - Accuracy: 100 / 300\n",
            "Epoch 3: [10/30] - Accuracy: 100 / 300\n",
            "Epoch 3: [11/30] - Accuracy: 100 / 300\n",
            "Epoch 3: [12/30] - Accuracy: 100 / 300\n",
            "Epoch 3: [13/30] - Accuracy: 100 / 300\n",
            "Epoch 3: [14/30] - Accuracy: 100 / 300\n",
            "Epoch 3: [15/30] - Accuracy: 142 / 300\n",
            "Epoch 3: [16/30] - Accuracy: 100 / 300\n",
            "Epoch 3: [17/30] - Accuracy: 100 / 300\n",
            "Epoch 3: [18/30] - Accuracy: 100 / 300\n",
            "Epoch 3: [19/30] - Accuracy: 100 / 300\n",
            "Epoch 3: [20/30] - Accuracy: 100 / 300\n",
            "Epoch 3: [21/30] - Accuracy: 100 / 300\n",
            "Epoch 3: [22/30] - Accuracy: 100 / 300\n",
            "Epoch 3: [23/30] - Accuracy: 100 / 300\n",
            "Epoch 3: [24/30] - Accuracy: 100 / 300\n",
            "Epoch 3: [25/30] - Accuracy: 100 / 300\n",
            "Epoch 3: [26/30] - Accuracy: 100 / 300\n",
            "Epoch 3: [27/30] - Accuracy: 170 / 300\n",
            "Epoch 3: [28/30] - Accuracy: 100 / 300\n",
            "Epoch 3: [29/30] - Accuracy: 100 / 300\n",
            "Epoch 3: [30/30] - Accuracy: 100 / 300\n",
            "Epoch 4: [1/30] - Accuracy: 100 / 300\n",
            "Epoch 4: [2/30] - Accuracy: 100 / 300\n",
            "Epoch 4: [3/30] - Accuracy: 100 / 300\n",
            "Epoch 4: [4/30] - Accuracy: 100 / 300\n",
            "Epoch 4: [5/30] - Accuracy: 100 / 300\n",
            "Epoch 4: [6/30] - Accuracy: 100 / 300\n",
            "Epoch 4: [7/30] - Accuracy: 163 / 300\n",
            "Epoch 4: [8/30] - Accuracy: 196 / 300\n",
            "Epoch 4: [9/30] - Accuracy: 100 / 300\n",
            "Epoch 4: [10/30] - Accuracy: 100 / 300\n",
            "Epoch 4: [11/30] - Accuracy: 104 / 300\n",
            "Epoch 4: [12/30] - Accuracy: 169 / 300\n",
            "Epoch 4: [13/30] - Accuracy: 100 / 300\n",
            "Epoch 4: [14/30] - Accuracy: 100 / 300\n",
            "Epoch 4: [15/30] - Accuracy: 100 / 300\n",
            "Epoch 4: [16/30] - Accuracy: 101 / 300\n",
            "Epoch 4: [17/30] - Accuracy: 100 / 300\n",
            "Epoch 4: [18/30] - Accuracy: 109 / 300\n",
            "Epoch 4: [19/30] - Accuracy: 159 / 300\n",
            "Epoch 4: [20/30] - Accuracy: 173 / 300\n",
            "Epoch 4: [21/30] - Accuracy: 97 / 300\n",
            "Epoch 4: [22/30] - Accuracy: 100 / 300\n",
            "Epoch 4: [23/30] - Accuracy: 260 / 300\n",
            "Epoch 4: [24/30] - Accuracy: 100 / 300\n",
            "Epoch 4: [25/30] - Accuracy: 116 / 300\n",
            "Epoch 4: [26/30] - Accuracy: 100 / 300\n",
            "Epoch 4: [27/30] - Accuracy: 100 / 300\n",
            "Epoch 4: [28/30] - Accuracy: 146 / 300\n",
            "Epoch 4: [29/30] - Accuracy: 100 / 300\n",
            "Epoch 4: [30/30] - Accuracy: 100 / 300\n",
            "Epoch 5: [1/30] - Accuracy: 103 / 300\n",
            "Epoch 5: [2/30] - Accuracy: 100 / 300\n",
            "Epoch 5: [3/30] - Accuracy: 128 / 300\n",
            "Epoch 5: [4/30] - Accuracy: 181 / 300\n",
            "Epoch 5: [5/30] - Accuracy: 180 / 300\n",
            "Epoch 5: [6/30] - Accuracy: 103 / 300\n",
            "Epoch 5: [7/30] - Accuracy: 192 / 300\n",
            "Epoch 5: [8/30] - Accuracy: 100 / 300\n",
            "Epoch 5: [9/30] - Accuracy: 232 / 300\n",
            "Epoch 5: [10/30] - Accuracy: 178 / 300\n",
            "Epoch 5: [11/30] - Accuracy: 148 / 300\n",
            "Epoch 5: [12/30] - Accuracy: 233 / 300\n",
            "Epoch 5: [13/30] - Accuracy: 149 / 300\n",
            "Epoch 5: [14/30] - Accuracy: 179 / 300\n",
            "Epoch 5: [15/30] - Accuracy: 103 / 300\n",
            "Epoch 5: [16/30] - Accuracy: 180 / 300\n",
            "Epoch 5: [17/30] - Accuracy: 143 / 300\n",
            "Epoch 5: [18/30] - Accuracy: 182 / 300\n",
            "Epoch 5: [19/30] - Accuracy: 176 / 300\n",
            "Epoch 5: [20/30] - Accuracy: 275 / 300\n",
            "Epoch 5: [21/30] - Accuracy: 214 / 300\n",
            "Epoch 5: [22/30] - Accuracy: 187 / 300\n",
            "Epoch 5: [23/30] - Accuracy: 137 / 300\n",
            "Epoch 5: [24/30] - Accuracy: 169 / 300\n",
            "Epoch 5: [25/30] - Accuracy: 183 / 300\n",
            "Epoch 5: [26/30] - Accuracy: 171 / 300\n",
            "Epoch 5: [27/30] - Accuracy: 145 / 300\n",
            "Epoch 5: [28/30] - Accuracy: 175 / 300\n",
            "Epoch 5: [29/30] - Accuracy: 177 / 300\n",
            "Epoch 5: [30/30] - Accuracy: 179 / 300\n",
            "Epoch 6: [1/30] - Accuracy: 270 / 300\n",
            "Epoch 6: [2/30] - Accuracy: 236 / 300\n",
            "Epoch 6: [3/30] - Accuracy: 192 / 300\n",
            "Epoch 6: [4/30] - Accuracy: 133 / 300\n",
            "Epoch 6: [5/30] - Accuracy: 128 / 300\n",
            "Epoch 6: [6/30] - Accuracy: 263 / 300\n",
            "Epoch 6: [7/30] - Accuracy: 239 / 300\n",
            "Epoch 6: [8/30] - Accuracy: 233 / 300\n",
            "Epoch 6: [9/30] - Accuracy: 173 / 300\n",
            "Epoch 6: [10/30] - Accuracy: 173 / 300\n",
            "Epoch 6: [11/30] - Accuracy: 234 / 300\n",
            "Epoch 6: [12/30] - Accuracy: 231 / 300\n",
            "Epoch 6: [13/30] - Accuracy: 273 / 300\n",
            "Epoch 6: [14/30] - Accuracy: 229 / 300\n",
            "Epoch 6: [15/30] - Accuracy: 203 / 300\n",
            "Epoch 6: [16/30] - Accuracy: 264 / 300\n",
            "Epoch 6: [17/30] - Accuracy: 274 / 300\n",
            "Epoch 6: [18/30] - Accuracy: 273 / 300\n",
            "Epoch 6: [19/30] - Accuracy: 243 / 300\n",
            "Epoch 6: [20/30] - Accuracy: 205 / 300\n",
            "Epoch 6: [21/30] - Accuracy: 217 / 300\n",
            "Epoch 6: [22/30] - Accuracy: 210 / 300\n",
            "Epoch 6: [23/30] - Accuracy: 177 / 300\n",
            "Epoch 6: [24/30] - Accuracy: 262 / 300\n",
            "Epoch 6: [25/30] - Accuracy: 271 / 300\n",
            "Epoch 6: [26/30] - Accuracy: 260 / 300\n",
            "Epoch 6: [27/30] - Accuracy: 253 / 300\n",
            "Epoch 6: [28/30] - Accuracy: 226 / 300\n",
            "Epoch 6: [29/30] - Accuracy: 278 / 300\n",
            "Epoch 6: [30/30] - Accuracy: 239 / 300\n",
            "Epoch 7: [1/30] - Accuracy: 259 / 300\n",
            "Epoch 7: [2/30] - Accuracy: 271 / 300\n",
            "Epoch 7: [3/30] - Accuracy: 251 / 300\n",
            "Epoch 7: [4/30] - Accuracy: 276 / 300\n",
            "Epoch 7: [5/30] - Accuracy: 261 / 300\n",
            "Epoch 7: [6/30] - Accuracy: 272 / 300\n",
            "Epoch 7: [7/30] - Accuracy: 274 / 300\n",
            "Epoch 7: [8/30] - Accuracy: 272 / 300\n",
            "Epoch 7: [9/30] - Accuracy: 276 / 300\n",
            "Epoch 7: [10/30] - Accuracy: 274 / 300\n",
            "Epoch 7: [11/30] - Accuracy: 275 / 300\n",
            "Epoch 7: [12/30] - Accuracy: 275 / 300\n",
            "Epoch 7: [13/30] - Accuracy: 276 / 300\n",
            "Epoch 7: [14/30] - Accuracy: 271 / 300\n",
            "Epoch 7: [15/30] - Accuracy: 257 / 300\n",
            "Epoch 7: [16/30] - Accuracy: 277 / 300\n",
            "Epoch 7: [17/30] - Accuracy: 275 / 300\n",
            "Epoch 7: [18/30] - Accuracy: 269 / 300\n",
            "Epoch 7: [19/30] - Accuracy: 267 / 300\n",
            "Epoch 7: [20/30] - Accuracy: 271 / 300\n",
            "Epoch 7: [21/30] - Accuracy: 276 / 300\n",
            "Epoch 7: [22/30] - Accuracy: 263 / 300\n",
            "Epoch 7: [23/30] - Accuracy: 277 / 300\n",
            "Epoch 7: [24/30] - Accuracy: 276 / 300\n",
            "Epoch 7: [25/30] - Accuracy: 275 / 300\n",
            "Epoch 7: [26/30] - Accuracy: 274 / 300\n",
            "Epoch 7: [27/30] - Accuracy: 277 / 300\n",
            "Epoch 7: [28/30] - Accuracy: 277 / 300\n",
            "Epoch 7: [29/30] - Accuracy: 277 / 300\n",
            "Epoch 7: [30/30] - Accuracy: 276 / 300\n",
            "Epoch 8: [1/30] - Accuracy: 272 / 300\n",
            "Epoch 8: [2/30] - Accuracy: 276 / 300\n",
            "Epoch 8: [3/30] - Accuracy: 274 / 300\n",
            "Epoch 8: [4/30] - Accuracy: 274 / 300\n",
            "Epoch 8: [5/30] - Accuracy: 278 / 300\n",
            "Epoch 8: [6/30] - Accuracy: 268 / 300\n",
            "Epoch 8: [7/30] - Accuracy: 274 / 300\n",
            "Epoch 8: [8/30] - Accuracy: 276 / 300\n",
            "Epoch 8: [9/30] - Accuracy: 277 / 300\n",
            "Epoch 8: [10/30] - Accuracy: 274 / 300\n",
            "Epoch 8: [11/30] - Accuracy: 274 / 300\n",
            "Epoch 8: [12/30] - Accuracy: 273 / 300\n",
            "Epoch 8: [13/30] - Accuracy: 274 / 300\n",
            "Epoch 8: [14/30] - Accuracy: 278 / 300\n",
            "Epoch 8: [15/30] - Accuracy: 274 / 300\n",
            "Epoch 8: [16/30] - Accuracy: 277 / 300\n",
            "Epoch 8: [17/30] - Accuracy: 277 / 300\n",
            "Epoch 8: [18/30] - Accuracy: 278 / 300\n",
            "Epoch 8: [19/30] - Accuracy: 273 / 300\n",
            "Epoch 8: [20/30] - Accuracy: 273 / 300\n",
            "Epoch 8: [21/30] - Accuracy: 266 / 300\n",
            "Epoch 8: [22/30] - Accuracy: 270 / 300\n",
            "Epoch 8: [23/30] - Accuracy: 275 / 300\n",
            "Epoch 8: [24/30] - Accuracy: 275 / 300\n",
            "Epoch 8: [25/30] - Accuracy: 277 / 300\n",
            "Epoch 8: [26/30] - Accuracy: 275 / 300\n",
            "Epoch 8: [27/30] - Accuracy: 274 / 300\n",
            "Epoch 8: [28/30] - Accuracy: 273 / 300\n",
            "Epoch 8: [29/30] - Accuracy: 277 / 300\n",
            "Epoch 8: [30/30] - Accuracy: 275 / 300\n",
            "Epoch 9: [1/30] - Accuracy: 274 / 300\n",
            "Epoch 9: [2/30] - Accuracy: 277 / 300\n",
            "Epoch 9: [3/30] - Accuracy: 274 / 300\n",
            "Epoch 9: [4/30] - Accuracy: 272 / 300\n",
            "Epoch 9: [5/30] - Accuracy: 274 / 300\n",
            "Epoch 9: [6/30] - Accuracy: 275 / 300\n",
            "Epoch 9: [7/30] - Accuracy: 273 / 300\n",
            "Epoch 9: [8/30] - Accuracy: 273 / 300\n",
            "Epoch 9: [9/30] - Accuracy: 272 / 300\n",
            "Epoch 9: [10/30] - Accuracy: 272 / 300\n",
            "Epoch 9: [11/30] - Accuracy: 271 / 300\n",
            "Epoch 9: [12/30] - Accuracy: 277 / 300\n",
            "Epoch 9: [13/30] - Accuracy: 278 / 300\n",
            "Epoch 9: [14/30] - Accuracy: 276 / 300\n",
            "Epoch 9: [15/30] - Accuracy: 276 / 300\n",
            "Epoch 9: [16/30] - Accuracy: 276 / 300\n",
            "Epoch 9: [17/30] - Accuracy: 269 / 300\n",
            "Epoch 9: [18/30] - Accuracy: 274 / 300\n",
            "Epoch 9: [19/30] - Accuracy: 276 / 300\n",
            "Epoch 9: [20/30] - Accuracy: 274 / 300\n",
            "Epoch 9: [21/30] - Accuracy: 277 / 300\n",
            "Epoch 9: [22/30] - Accuracy: 275 / 300\n",
            "Epoch 9: [23/30] - Accuracy: 275 / 300\n",
            "Epoch 9: [24/30] - Accuracy: 277 / 300\n",
            "Epoch 9: [25/30] - Accuracy: 275 / 300\n",
            "Epoch 9: [26/30] - Accuracy: 277 / 300\n",
            "Epoch 9: [27/30] - Accuracy: 275 / 300\n",
            "Epoch 9: [28/30] - Accuracy: 274 / 300\n",
            "Epoch 9: [29/30] - Accuracy: 278 / 300\n",
            "Epoch 9: [30/30] - Accuracy: 275 / 300\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD4CAYAAADvsV2wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}