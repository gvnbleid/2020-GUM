{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MGU_projekt_1a.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKHlueYTMrFb",
        "colab_type": "text"
      },
      "source": [
        "# Sieć neuronowa\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "635Py62nMhC0",
        "colab_type": "text"
      },
      "source": [
        "## Funkcje aktywacji"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZW2F_-9Mck-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1.0 - sigmoid(x))\n",
        "\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(x, 0)\n",
        "\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1.0, 0.0)\n",
        "\n",
        "\n",
        "def identity(x):\n",
        "    return x\n",
        "\n",
        "\n",
        "def identity_derivative(x):\n",
        "    return np.array([[1] for i in x])\n",
        "\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "\n",
        "def tanh_derivative(x):\n",
        "    return 1.0 - np.square(np.tanh(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ul6Q6T2F4Y-d",
        "colab_type": "text"
      },
      "source": [
        "## Pochodne funkcji straty"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2j29Du94e1F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def quadratic_cost_derivative(output_activations, y):\n",
        "    return (output_activations - y)\n",
        "\n",
        "def cross_entropy_cost_derivative(output_activations, y):\n",
        "    return (output_activations - y)/((1-output_activations) * output_activations)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXzf8CRcNL8q",
        "colab_type": "text"
      },
      "source": [
        "## Klasa sieci"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gb4EE9VbNSMS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import codecs\n",
        "from enum import Enum\n",
        "\n",
        "class TaskType(Enum):\n",
        "    Classification = 1\n",
        "    Regression = 2\n",
        "\n",
        "class NeuralNetwork:\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      layer_sizes,\n",
        "      activation_function,\n",
        "      activation_function_deriv,\n",
        "      output_function,\n",
        "      output_function_deriv,\n",
        "      cost_function_deriv,\n",
        "      using_bias,\n",
        "      taskType,\n",
        "      seed = None\n",
        "    ):\n",
        "    self.layer_num = len(layer_sizes)\n",
        "    self.layer_sizes = layer_sizes\n",
        "\n",
        "    if(seed != None):\n",
        "      np.random.seed(seed)\n",
        "\n",
        "    self.edges_weights = [np.random.randn(x, y) for x, y in zip(layer_sizes[1:], layer_sizes[:-1])]\n",
        "    self.biases_weights = [np.random.rand(x, 1) for x in layer_sizes[1:]]\n",
        "    self.activation_function = activation_function\n",
        "    self.activation_function_deriv = activation_function_deriv\n",
        "    self.output_function = output_function\n",
        "    self.output_function_deriv = output_function_deriv\n",
        "    self.cost_function_deriv = cost_function_deriv\n",
        "    self.using_bias = using_bias\n",
        "    self.weighted_sums = []\n",
        "    self.activations = []\n",
        "    self.last_delta_weight = None\n",
        "    self.last_delta_biases = None\n",
        "    self.moment_factor = None\n",
        "    self.learning_factor = None\n",
        "    self.batch_size = None\n",
        "    self.epoch_num = None\n",
        "    self.history = []\n",
        "    self.taskType = taskType\n",
        "    self.classification_area_probing = []\n",
        "\n",
        "  def backpropagate(self, net_input, desired_output):\n",
        "    delta_weight = [np.zeros(es.shape) for es in self.edges_weights]\n",
        "    delta_biases = [np.zeros(bs.shape) for bs in self.biases_weights]\n",
        "\n",
        "    self.activations = []\n",
        "    self.weighted_sums = []\n",
        "\n",
        "    if(self.using_bias):\n",
        "      net_output = self.feed_forward(net_input)\n",
        "    else:\n",
        "      net_output = self.feed_forward_no_bias(net_input)\n",
        "\n",
        "    cost_per_node = self.cost_function_deriv(net_output, desired_output)\n",
        "    ofd = self.output_function_deriv(self.weighted_sums[-1])\n",
        "    d_per_node = cost_per_node * ofd\n",
        "    \n",
        "    delta_weight[-1] = np.dot(d_per_node, self.activations[-2].transpose())\n",
        "    if(self.using_bias):\n",
        "      delta_biases[-1] = d_per_node;\n",
        "\n",
        "    for l in range(2, self.layer_num):\n",
        "      ws = self.weighted_sums[-l]\n",
        "      afd = self.activation_function_deriv(ws)\n",
        "      d_per_node = np.dot(self.edges_weights[-l+1].transpose(), d_per_node) * afd\n",
        "      \n",
        "      delta_weight[-l] = np.dot(d_per_node, self.activations[-l-1].transpose())\n",
        "      \n",
        "      if(self.using_bias):\n",
        "        delta_biases[-l] = d_per_node\n",
        "      \n",
        "\n",
        "    return (delta_weight, delta_biases)\n",
        "\n",
        "  def feed_forward(self, net_input):\n",
        "    tmp_output = net_input\n",
        "    self.activations.append(net_input)\n",
        "\n",
        "    for bias, layer_weights in zip(self.biases_weights[:-1], self.edges_weights[:-1]):\n",
        "      layer_ws = np.dot(layer_weights, tmp_output) + bias\n",
        "      self.weighted_sums.append(layer_ws)\n",
        "      layer_activs = self.activation_function(layer_ws)\n",
        "      self.activations.append(layer_activs)\n",
        "      tmp_output = layer_activs\n",
        "\n",
        "    layer_ws = np.dot(self.edges_weights[-1], tmp_output) + self.biases_weights[-1]\n",
        "    self.weighted_sums.append(layer_ws)\n",
        "    layer_activs = self.output_function(layer_ws)\n",
        "    self.activations.append(layer_activs)\n",
        "    return layer_activs\n",
        "\n",
        "  def feed_forward_no_bias(self, net_input):\n",
        "    tmp_output = net_input\n",
        "    self.activations.append(net_input)\n",
        "\n",
        "    for layer_weights in self.edges_weights[:-1]:\n",
        "      layer_ws = np.dot(layer_weights, tmp_output)\n",
        "      self.weighted_sums.append(layer_ws)\n",
        "      layer_activs = self.activation_function(layer_ws)\n",
        "      self.activations.append(layer_activs)\n",
        "      tmp_output = layer_activs\n",
        "\n",
        "    layer_ws = np.dot(self.edges_weights[-1], tmp_output)\n",
        "    self.weighted_sums.append(layer_ws)\n",
        "    layer_activs = self.output_function(layer_ws)\n",
        "    self.activations.append(layer_activs)\n",
        "    return layer_activs\n",
        "\n",
        "  def learn(self, training_set, test_set, learning_factor, moment_factor, batch_size, epoch_num, probe=None):\n",
        "    training_set_cp = training_set\n",
        "\n",
        "    self.moment_factor = moment_factor\n",
        "    self.learning_factor = learning_factor\n",
        "    self.last_delta_biases = None\n",
        "    self.last_delta_weight = None\n",
        "    self.batch_size = batch_size\n",
        "    self.epoch_num = epoch_num\n",
        "\n",
        "    \n",
        "    for i in range(epoch_num):\n",
        "      np.random.shuffle(training_set_cp)\n",
        "      batches = [training_set_cp[i * batch_size:(i + 1) * batch_size] \n",
        "               for i in range((len(training_set_cp) + batch_size - 1) // batch_size )]\n",
        "      \n",
        "      for batch, batch_id in zip(batches, range(1, 1 + len(batches))):\n",
        "        sum_dw = [np.zeros(ew.shape) for ew in self.edges_weights]\n",
        "        sum_db = [np.zeros(bs.shape) for bs in self.biases_weights]\n",
        "        for ni, dno in batch:\n",
        "\n",
        "          dw, db = self.backpropagate(ni, dno)\n",
        "          sum_dw = [sdw + ndw for sdw, ndw in zip(sum_dw, dw)]\n",
        "          if(self.using_bias):\n",
        "            sum_db = [sdb + ndb for sdb, ndb in zip(sum_db, db)]\n",
        "        \n",
        "        if(self.last_delta_weight != None):\n",
        "          sum_dw = [self.moment_factor * ldw + (1 - self.moment_factor) * sdw \n",
        "                    for sdw, ldw in zip(sum_dw, self.last_delta_weight)]\n",
        "          if(self.using_bias):\n",
        "            sum_db = [self.moment_factor * ldb + (1 - self.moment_factor) * sdb \n",
        "                      for sdb, ldb in zip(sum_db, self.last_delta_biases)]\n",
        "\n",
        "        self.edges_weights = [ew - (learning_factor / len(batch)) * sdw for ew, sdw in zip(self.edges_weights, sum_dw)]\n",
        "        \n",
        "        if(self.using_bias):\n",
        "          self.biases_weights = [bw - (learning_factor / len(batch)) * sdb for bw, sdb in zip(self.biases_weights, sum_db)]\n",
        "\n",
        "        self.last_delta_biases = sum_db\n",
        "        self.last_delta_weight = sum_dw\n",
        "\n",
        "      test_acc = self.test_accuracy(test_set)\n",
        "      train_acc = self.test_accuracy(training_set)\n",
        "\n",
        "      if(self.taskType == TaskType.Classification):\n",
        "        print(f\"Epoch {i}: [{batch_id}/{len(batches)}] - Accuracy: {train_acc / len(training_set)}\")\n",
        "      else:\n",
        "        print(f\"Epoch {i}: [{batch_id}/{len(batches)}] - MSE: {test_acc}\")\n",
        "\n",
        "      self.loss = 0\n",
        "\n",
        "      self.take_snapshot(f'Epoch {i}: ', train_acc, test_acc)\n",
        "\n",
        "      if probe != None:\n",
        "        self.classification_area_probing.append(self.classify(probe))\n",
        "        \n",
        "  def test_accuracy(self, test_data):\n",
        "    if(self.taskType == TaskType.Classification):\n",
        "      return self.validate_classification(test_data)\n",
        "    else:\n",
        "      return self.validate_regression(test_data)\n",
        "  \n",
        "  def validate_classification(self, data):\n",
        "      correct = 0\n",
        "      for x, y in data:\n",
        "          expected = np.argmax(y)\n",
        "          result = np.argmax(self.feed_forward(x))\n",
        "          if expected == result:\n",
        "              correct = correct + 1\n",
        "      return correct\n",
        "  \n",
        "  def validate_regression(self, data):\n",
        "    error = 0\n",
        "    for x, y in data:\n",
        "        error = error + np.square(self.feed_forward(x)-y).mean()\n",
        "    return error / len(data)\n",
        "\n",
        "  def classify(self, test_data):\n",
        "        results = []\n",
        "        for x in test_data:\n",
        "            classification = np.argmax(self.feed_forward(x))\n",
        "            results.append(classification)\n",
        "        return results\n",
        "      \n",
        "  def take_snapshot(self, name, train_accuracy=None, test_accuracy=None):\n",
        "    if(self.using_bias):\n",
        "      dump = dict(biases=[bias.tolist() for bias in self.biases_weights],\n",
        "                  weight_matrix=[weight.tolist() for weight in self.edges_weights],\n",
        "                  name=name,\n",
        "                  train_fitness=train_accuracy,\n",
        "                  test_fitness=test_accuracy)\n",
        "    else:\n",
        "      dump = dict(weight_matrix=[weight.tolist() for weight in self.edges_weights],\n",
        "                  name=name,\n",
        "                  train_fitness=train_accuracy,\n",
        "                  test_fitness=test_accuracy)\n",
        "      \n",
        "    self.history.append(dump)\n",
        "\n",
        "  def save_history(self, out_filepath):\n",
        "    network_info = dict(\n",
        "        sizes=self.layer_sizes, learning_factor=self.learning_factor,\n",
        "        moment_factor=self.moment_factor,\n",
        "        epoch_num=self.epoch_num, batch_size=self.batch_size,\n",
        "        activation_function=self.activation_function.__name__,\n",
        "        output_function=self.output_function.__name__)\n",
        "    json.dump(dict(history=self.history, network_info=network_info),\n",
        "              codecs.open(out_filepath, 'w', encoding='utf-8'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3g1s3G3468Ga",
        "colab_type": "text"
      },
      "source": [
        "## Podpięcie dysku i wczytywanie danych"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9kob-tk7C5h",
        "colab_type": "text"
      },
      "source": [
        "### Podpięcie dysku"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlPVGpq77Ewk",
        "colab_type": "code",
        "outputId": "f27d50ca-a597-4c92-ed9d-74ea730137d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "BASE_DIR = '/content/gdrive/My Drive/DL2020/Projekt1a'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-naFIiYM7Gi8",
        "colab_type": "text"
      },
      "source": [
        "### Ładowanie danych"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awnuWVRX7Ih7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def load_classification_wrapper(path, name, train_size, test_size=None):\n",
        "    if test_size == None:\n",
        "        test_size = train_size\n",
        "\n",
        "    train = load_classification(os.path.join(\n",
        "        path, f'{name}.train.{train_size}.csv'), ['x', 'y'])\n",
        "    test = load_classification(os.path.join(\n",
        "        path, f'{name}.test.{test_size}.csv'), ['x', 'y'])\n",
        "    return (train, test)\n",
        "\n",
        "\n",
        "def load_classification(csv_filename, argument_column_names, class_column_name='cls'):\n",
        "    df = pd.read_csv(csv_filename)\n",
        "\n",
        "    output_layer_neuron_count = len(df[class_column_name].unique())\n",
        "    input_layer_neuron_count = len(argument_column_names)\n",
        "\n",
        "    training_data = []\n",
        "\n",
        "    for row in df.itertuples():\n",
        "        x = np.zeros((input_layer_neuron_count, 1))\n",
        "\n",
        "        for i in range(len(argument_column_names)):\n",
        "            x[i] = getattr(row, argument_column_names[i])\n",
        "\n",
        "        y = vectorize_class(getattr(row, class_column_name),\n",
        "                            output_layer_neuron_count)\n",
        "\n",
        "        training_data.append((x, y))\n",
        "    return training_data\n",
        "\n",
        "def load_regression(csv_filename, input_column_names, output_column_names):\n",
        "    df = pd.read_csv(csv_filename)\n",
        "\n",
        "    output_layer_neuron_count = len(output_column_names)\n",
        "    input_layer_neuron_count = len(input_column_names)\n",
        "\n",
        "    training_data = []\n",
        "\n",
        "    for row in df.itertuples():\n",
        "        x = np.zeros((input_layer_neuron_count, 1))\n",
        "        y = np.zeros((output_layer_neuron_count, 1))\n",
        "\n",
        "        for i in range(len(input_column_names)):\n",
        "            x[i] = getattr(row, input_column_names[i])\n",
        "\n",
        "        for i in range(len(output_column_names)):\n",
        "            y[i] = getattr(row, output_column_names[i])\n",
        "\n",
        "        training_data.append((x, y))\n",
        "    return training_data\n",
        "\n",
        "def load_regression_wrapper(path, name, train_size, test_size=None):\n",
        "    if test_size == None:\n",
        "        test_size = train_size\n",
        "\n",
        "    train = load_regression(os.path.join(\n",
        "        path, f'{name}.train.{train_size}.csv'), ['x'], ['y'])\n",
        "    test = load_regression(os.path.join(\n",
        "        path, f'{name}.test.{test_size}.csv'), ['x'], ['y'])\n",
        "    return (train, test)\n",
        "\n",
        "\n",
        "def vectorize_class(class_id, class_length):\n",
        "    # assume that classification starts at 1 to class_length, e.g. 1,2,3,4\n",
        "    class_id = class_id - 1\n",
        "    y = np.zeros((class_length, 1))\n",
        "    y[class_id] = 1\n",
        "    return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80unqXUN7-vW",
        "colab_type": "text"
      },
      "source": [
        "## Odpalenie sieci"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vq0gcLoPrT0q",
        "colab_type": "text"
      },
      "source": [
        "### Klasyfikacja"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnNerGan8BYI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "train, test = load_classification_wrapper(BASE_DIR + r'/data/classification', 'data.three_gauss', 100)\n",
        "inp_lay_size = len(train[0][0])\n",
        "out_lay_size = len(train[0][1])\n",
        "np.random.seed(11)\n",
        "nn = NeuralNetwork([inp_lay_size, 8, out_lay_size], sigmoid, sigmoid_derivative, sigmoid, sigmoid_derivative, quadratic_cost_derivative, True, TaskType.Classification)\n",
        "nn.learn(train, test, 0.9, 0.3, 20, 500)\n",
        "\n",
        "now = datetime.now()\n",
        "current_time = now.strftime(\"%H:%M:%S\")\n",
        "\n",
        "nn.save_history(BASE_DIR + f'/results/classification_{current_time}.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhSifZGurZo_",
        "colab_type": "text"
      },
      "source": [
        "### Regresja"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gF4_TE8YxtMW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "train, test = load_regression_wrapper(BASE_DIR + r'/data/regression', 'data.activation', 100)\n",
        "inp_lay_size = len(train[0][0])\n",
        "out_lay_size = len(train[0][1])\n",
        "np.random.seed(0)\n",
        "nn = NeuralNetwork([inp_lay_size, 5, out_lay_size], sigmoid, sigmoid_derivative, identity, identity_derivative, quadratic_cost_derivative, True, TaskType.Regression)\n",
        "nn.learn(train, test, 0.1, 0, 20, 100)\n",
        "\n",
        "now = datetime.now()\n",
        "current_time = now.strftime(\"%H:%M:%S\")\n",
        "\n",
        "nn.save_history(BASE_DIR + f'/results/regression_{current_time}.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOmG42C3c3QK",
        "colab_type": "text"
      },
      "source": [
        "## Testy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zISQ5_Myc6pJ",
        "colab_type": "text"
      },
      "source": [
        "### Feed forward"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE5bhR9PdANF",
        "colab_type": "code",
        "outputId": "02874af5-118b-422d-f5a7-815d2ab6547d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "np.random.seed(0)\n",
        "l_sizes = [2,2,2]\n",
        "w = [np.random.randn(x, y) * 0.01 for x, y in zip(l_sizes[1:], l_sizes[:-1])]\n",
        "b = [np.random.rand(x, 1) for x in l_sizes[1:]]\n",
        "\n",
        "inp = np.array([[1], [1]])\n",
        "res2 = inp\n",
        "for ww, bb in zip(w, b):\n",
        "  res2 = identity(np.dot(ww, res2) + bb)\n",
        "\n",
        "net = NeuralNetwork([2, 2, 2], identity, identity_derivative, identity, identity_derivative, quadratic_cost_derivative, True, TaskType.Classification, 0)\n",
        "res = net.feed_forward(inp)\n",
        "print(res)\n",
        "print(res2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.80606424]\n",
            " [0.53762709]]\n",
            "[[0.80606424]\n",
            " [0.53762709]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jG41OXvmV7Q",
        "colab_type": "text"
      },
      "source": [
        "### Błąd = 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyRgIUphmUC4",
        "colab_type": "code",
        "outputId": "1bc0f200-3c0f-4f89-c268-34c5327339dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "np.random.seed(0)\n",
        "l_sizes = [2,2,2]\n",
        "output = np.array([[0.80606424], [0.53762709]])\n",
        "inp = np.array([[1], [1]])\n",
        "\n",
        "net = NeuralNetwork([2, 2, 2], identity, identity_derivative, identity, identity_derivative, quadratic_cost_derivative, True, TaskType.Classification, 0)\n",
        "w, b = net.backpropagate(inp, output)\n",
        "print(w)\n",
        "print(b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[ 1.28539538e-11,  1.28539538e-11],\n",
            "       [-9.31866041e-12, -9.31866041e-12]]), array([[ 1.05391157e-09,  4.44578668e-10],\n",
            "       [-7.38599336e-10, -3.11568368e-10]])]\n",
            "[array([[ 1.28539538e-11],\n",
            "       [-9.31866041e-12]]), array([[ 1.06962994e-09],\n",
            "       [-7.49615037e-10]])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5AAfuVYwtc2",
        "colab_type": "text"
      },
      "source": [
        "### Uczenie nauczonej sieci"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Krur2o9Yw3xF",
        "colab_type": "code",
        "outputId": "d4cb1b74-88fd-4dac-e283-31ab28bcd336",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "np.random.seed(0)\n",
        "l_sizes = [2,2,2]\n",
        "w = [np.random.randn(x, y) * 0.01 for x, y in zip(l_sizes[1:], l_sizes[:-1])]\n",
        "b = [np.random.rand(x, 1) for x in l_sizes[1:]]\n",
        "\n",
        "training_set = [(np.array([[1], [1]]), np.array([[0.80606424], [0.53762709]]))]\n",
        "test_set = [(np.array([[1], [1]]), np.array([[0.80606424], [0.53762709]]))]\n",
        "\n",
        "net = NeuralNetwork([2, 2, 2], identity, identity_derivative, identity, identity_derivative, quadratic_cost_derivative, True, TaskType.Classification, 0)\n",
        "net.learn(training_set, test_set, 0.1, 0.1, 1, 1)\n",
        "print(net.feed_forward(np.array([[1], [1]])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: [1/1] - Train set accuracy: 1 / 1\n",
            "Epoch 0: [1/1] - Test set accuracy: 1\n",
            "[[0.80606424]\n",
            " [0.53762709]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZVyAWbtbwTK",
        "colab_type": "text"
      },
      "source": [
        "## Wyniki"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AK8cQC16b7pV",
        "colab_type": "text"
      },
      "source": [
        "### Wyniki klasyfikacji"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alc9vAD0cW6B",
        "colab_type": "text"
      },
      "source": [
        "#### Wizualizacja zbioru uczącego oraz rezultatów klasyfikacji"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jj6pFqrBclBY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as pyplot\n",
        "\n",
        "def create_filename(figure_name, data_name, epochs, batch_size):\n",
        "    return BASE_DIR + f'/results/classification/{figure_name}__{data_name.split(\".\")[1]}__{epochs}_{batch_size}.png'\n",
        "\n",
        "def create_heatmap(min_x, max_x, min_y, max_y, count_x, count_y):\n",
        "    step_x = (max_x - min_x) / count_x\n",
        "    step_y = (max_y - min_y) / count_y\n",
        "    probes_x = []\n",
        "    probes_y = []\n",
        "    probes_input = []\n",
        "    for x_i in range(count_x):\n",
        "        for y_i in range(count_y):\n",
        "            x = min_x + x_i * step_x\n",
        "            y = min_y + y_i * step_y\n",
        "            probes_x.append(x)\n",
        "            probes_y.append(y)\n",
        "            input_x = np.zeros((2, 1))\n",
        "            input_x[0] = x\n",
        "            input_x[1] = y\n",
        "            probes_input.append(input_x)\n",
        "    return probes_x, probes_y, probes_input\n",
        "\n",
        "def process(seed, hidden_layer_sizes, hidden_layers_activation_function, hidden_layers_activation_function_prime, output_layer_activation_function, output_layer_activation_function_prime, cost_derivative, is_bias_enabled, path, data_name, train_size,  epochs, batch_size, eta, alpha, test_size=None):\n",
        "    global nn\n",
        "    train_data, test_data = load_classification_wrapper(path, data_name, train_size, test_size)\n",
        "    np.random.seed(seed)\n",
        "    sizes = [len(train_data[0][0])]\n",
        "    sizes.extend(hidden_layer_sizes)\n",
        "    sizes.append(len(train_data[0][1]))\n",
        "    nn = NeuralNetwork(sizes, hidden_layers_activation_function, hidden_layers_activation_function_prime, output_layer_activation_function, output_layer_activation_function_prime, cost_derivative, is_bias_enabled, TaskType.Classification)\n",
        "\n",
        "    x = [entry[0][0][0] for entry in test_data]\n",
        "    y = [entry[0][1][0] for entry in test_data]\n",
        "\n",
        "    probes_x, probes_y, probes_input = create_heatmap(\n",
        "        min(x), max(x), min(y), max(y), 300, 300)\n",
        "\n",
        "    nn.learn(train_data, test_data, eta, alpha, batch_size, epochs, probe=probes_input)\n",
        "    \n",
        "    # classification points visualization\n",
        "    valid_classification = [np.argmax(entry[1]) for entry in test_data]\n",
        "    net_classification = nn.classify([x for (x, y) in test_data])\n",
        "    x = [entry[0][0][0] for entry in test_data]\n",
        "    y = [entry[0][1][0] for entry in test_data]\n",
        "\n",
        "    pyplot.scatter(x=x, y=y, c=net_classification)\n",
        "    pyplot.axis('equal')\n",
        "    pyplot.savefig(create_filename('net-classification', data_name, epochs, batch_size), dpi=150)\n",
        "    pyplot.show()\n",
        "    pyplot.clf()\n",
        "\n",
        "    # classificaiton area probing\n",
        "\n",
        "    for probe_classification, epoch in zip(nn.classification_area_probing, range(len(nn.classification_area_probing))):\n",
        "        pyplot.scatter(x=probes_x, y=probes_y, c=probe_classification)\n",
        "        pyplot.axis('equal')\n",
        "        pyplot.savefig(create_filename(f'probing-classification-{epoch}', data_name, epochs, batch_size), dpi=150)\n",
        "        pyplot.show()\n",
        "        pyplot.clf()\n",
        "\n",
        "process(0, [5], sigmoid, sigmoid_derivative, sigmoid, sigmoid_derivative, quadratic_cost_derivative, True, BASE_DIR + r'/data/classification', 'data.three_gauss', 100, 10, 10, 0.9, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}