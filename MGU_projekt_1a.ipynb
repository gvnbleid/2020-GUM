{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MGU_projekt_1a.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "80unqXUN7-vW",
        "KOmG42C3c3QK",
        "aZVyAWbtbwTK"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKHlueYTMrFb",
        "colab_type": "text"
      },
      "source": [
        "# Sieć neuronowa\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "635Py62nMhC0",
        "colab_type": "text"
      },
      "source": [
        "## Funkcje aktywacji"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZW2F_-9Mck-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1.0 - sigmoid(x))\n",
        "\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(x, 0)\n",
        "\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1.0, 0.0)\n",
        "\n",
        "\n",
        "def identity(x):\n",
        "    return x\n",
        "\n",
        "\n",
        "def identity_derivative(x):\n",
        "    return np.array([[1] for i in x])\n",
        "\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "\n",
        "def tanh_derivative(x):\n",
        "    return 1.0 - np.square(np.tanh(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ul6Q6T2F4Y-d",
        "colab_type": "text"
      },
      "source": [
        "## Pochodne funkcji straty"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2j29Du94e1F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def quadratic_cost_derivative(output_activations, y):\n",
        "    return (output_activations - y)\n",
        "\n",
        "def cross_entropy_cost_derivative(output_activations, y):\n",
        "    return (output_activations - y)/((1-output_activations) * output_activations)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXzf8CRcNL8q",
        "colab_type": "text"
      },
      "source": [
        "## Klasa sieci"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gb4EE9VbNSMS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import codecs\n",
        "from enum import Enum\n",
        "\n",
        "class TaskType(Enum):\n",
        "    Classification = 1\n",
        "    Regression = 2\n",
        "\n",
        "class NeuralNetwork:\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      layer_sizes,\n",
        "      activation_function,\n",
        "      activation_function_deriv,\n",
        "      output_function,\n",
        "      output_function_deriv,\n",
        "      cost_function_deriv,\n",
        "      using_bias,\n",
        "      taskType,\n",
        "      seed = None\n",
        "    ):\n",
        "    self.layer_num = len(layer_sizes)\n",
        "    self.layer_sizes = layer_sizes\n",
        "    \n",
        "    self.seed = 0\n",
        "    if(seed != None):\n",
        "      self.seed = np.random.seed(seed)\n",
        "\n",
        "    self.edges_weights = [np.random.randn(x, y) for x, y in zip(layer_sizes[1:], layer_sizes[:-1])]\n",
        "    self.biases_weights = [np.random.rand(x, 1) for x in layer_sizes[1:]]\n",
        "    self.activation_function = activation_function\n",
        "    self.activation_function_deriv = activation_function_deriv\n",
        "    self.output_function = output_function\n",
        "    self.output_function_deriv = output_function_deriv\n",
        "    self.cost_function_deriv = cost_function_deriv\n",
        "    self.using_bias = using_bias\n",
        "    self.weighted_sums = []\n",
        "    self.activations = []\n",
        "    self.last_delta_weight = None\n",
        "    self.last_delta_biases = None\n",
        "    self.moment_factor = None\n",
        "    self.learning_factor = None\n",
        "    self.batch_size = None\n",
        "    self.epoch_num = None\n",
        "    self.history = []\n",
        "    self.taskType = taskType\n",
        "    self.classification_area_probing = []\n",
        "    self.best_acc = 0\n",
        "    self.best_epoch = 0\n",
        "\n",
        "  def backpropagate(self, net_input, desired_output):\n",
        "    delta_weight = [np.zeros(es.shape) for es in self.edges_weights]\n",
        "    delta_biases = [np.zeros(bs.shape) for bs in self.biases_weights]\n",
        "\n",
        "    self.activations = []\n",
        "    self.weighted_sums = []\n",
        "\n",
        "    if(self.using_bias):\n",
        "      net_output = self.feed_forward(net_input)\n",
        "    else:\n",
        "      net_output = self.feed_forward_no_bias(net_input)\n",
        "\n",
        "    cost_per_node = self.cost_function_deriv(net_output, desired_output)\n",
        "    ofd = self.output_function_deriv(self.weighted_sums[-1])\n",
        "    d_per_node = cost_per_node * ofd\n",
        "    \n",
        "    delta_weight[-1] = np.dot(d_per_node, self.activations[-2].transpose())\n",
        "    if(self.using_bias):\n",
        "      delta_biases[-1] = d_per_node;\n",
        "\n",
        "    for l in range(2, self.layer_num):\n",
        "      ws = self.weighted_sums[-l]\n",
        "      afd = self.activation_function_deriv(ws)\n",
        "      d_per_node = np.dot(self.edges_weights[-l+1].transpose(), d_per_node) * afd\n",
        "      \n",
        "      delta_weight[-l] = np.dot(d_per_node, self.activations[-l-1].transpose())\n",
        "      \n",
        "      if(self.using_bias):\n",
        "        delta_biases[-l] = d_per_node\n",
        "      \n",
        "\n",
        "    return (delta_weight, delta_biases)\n",
        "\n",
        "  def feed_forward(self, net_input):\n",
        "    tmp_output = net_input\n",
        "    self.activations.append(net_input)\n",
        "\n",
        "    for bias, layer_weights in zip(self.biases_weights[:-1], self.edges_weights[:-1]):\n",
        "      layer_ws = np.dot(layer_weights, tmp_output) + bias\n",
        "      self.weighted_sums.append(layer_ws)\n",
        "      layer_activs = self.activation_function(layer_ws)\n",
        "      self.activations.append(layer_activs)\n",
        "      tmp_output = layer_activs\n",
        "\n",
        "    layer_ws = np.dot(self.edges_weights[-1], tmp_output) + self.biases_weights[-1]\n",
        "    self.weighted_sums.append(layer_ws)\n",
        "    layer_activs = self.output_function(layer_ws)\n",
        "    self.activations.append(layer_activs)\n",
        "    return layer_activs\n",
        "\n",
        "  def feed_forward_no_bias(self, net_input):\n",
        "    tmp_output = net_input\n",
        "    self.activations.append(net_input)\n",
        "\n",
        "    for layer_weights in self.edges_weights[:-1]:\n",
        "      layer_ws = np.dot(layer_weights, tmp_output)\n",
        "      self.weighted_sums.append(layer_ws)\n",
        "      layer_activs = self.activation_function(layer_ws)\n",
        "      self.activations.append(layer_activs)\n",
        "      tmp_output = layer_activs\n",
        "\n",
        "    layer_ws = np.dot(self.edges_weights[-1], tmp_output)\n",
        "    self.weighted_sums.append(layer_ws)\n",
        "    layer_activs = self.output_function(layer_ws)\n",
        "    self.activations.append(layer_activs)\n",
        "    return layer_activs\n",
        "\n",
        "  def learn(self, training_set, test_set, learning_factor, moment_factor, batch_size, epoch_num, probe=None):\n",
        "    training_set_cp = training_set\n",
        "\n",
        "    self.moment_factor = moment_factor\n",
        "    self.learning_factor = learning_factor\n",
        "    self.last_delta_biases = None\n",
        "    self.last_delta_weight = None\n",
        "    self.batch_size = batch_size\n",
        "    self.epoch_num = epoch_num\n",
        "\n",
        "    \n",
        "    for i in range(epoch_num):\n",
        "      np.random.shuffle(training_set_cp)\n",
        "      batches = [training_set_cp[i * batch_size:(i + 1) * batch_size] \n",
        "               for i in range((len(training_set_cp) + batch_size - 1) // batch_size )]\n",
        "      \n",
        "      for batch, batch_id in zip(batches, range(1, 1 + len(batches))):\n",
        "        sum_dw = [np.zeros(ew.shape) for ew in self.edges_weights]\n",
        "        sum_db = [np.zeros(bs.shape) for bs in self.biases_weights]\n",
        "        for ni, dno in batch:\n",
        "\n",
        "          dw, db = self.backpropagate(ni, dno)\n",
        "          sum_dw = [sdw + ndw for sdw, ndw in zip(sum_dw, dw)]\n",
        "          if(self.using_bias):\n",
        "            sum_db = [sdb + ndb for sdb, ndb in zip(sum_db, db)]\n",
        "        \n",
        "        if(self.last_delta_weight != None):\n",
        "          sum_dw = [self.moment_factor * ldw + (1 - self.moment_factor) * sdw \n",
        "                    for sdw, ldw in zip(sum_dw, self.last_delta_weight)]\n",
        "          if(self.using_bias):\n",
        "            sum_db = [self.moment_factor * ldb + (1 - self.moment_factor) * sdb \n",
        "                      for sdb, ldb in zip(sum_db, self.last_delta_biases)]\n",
        "\n",
        "        self.edges_weights = [ew - (learning_factor / len(batch)) * sdw for ew, sdw in zip(self.edges_weights, sum_dw)]\n",
        "        \n",
        "        if(self.using_bias):\n",
        "          self.biases_weights = [bw - (learning_factor / len(batch)) * sdb for bw, sdb in zip(self.biases_weights, sum_db)]\n",
        "\n",
        "        self.last_delta_biases = sum_db\n",
        "        self.last_delta_weight = sum_dw\n",
        "\n",
        "      test_acc = self.test_accuracy(test_set)\n",
        "      train_acc = self.test_accuracy(training_set)\n",
        "\n",
        "      if(self.taskType == TaskType.Classification):\n",
        "        print(f\"Epoch {i}, Accuracy: {test_acc}\")\n",
        "        if(test_acc > self.best_acc):\n",
        "          self.best_acc = test_acc\n",
        "          self.best_epoch = i\n",
        "      else:\n",
        "        print(f\"Epoch {i}, MSE: {test_acc}\")\n",
        "\n",
        "\n",
        "      self.take_snapshot(f'Epoch {i}: ', train_acc, test_acc)\n",
        "\n",
        "      if probe != None:\n",
        "        self.classification_area_probing.append(self.classify(probe))\n",
        "        \n",
        "  def test_accuracy(self, test_data):\n",
        "    if(self.taskType == TaskType.Classification):\n",
        "      return self.validate_classification(test_data)\n",
        "    else:\n",
        "      return self.validate_regression(test_data)\n",
        "  \n",
        "  def validate_classification(self, data):\n",
        "      correct = 0\n",
        "      for x, y in data:\n",
        "          expected = np.argmax(y)\n",
        "          result = np.argmax(self.feed_forward(x))\n",
        "          if expected == result:\n",
        "              correct = correct + 1\n",
        "      return correct / len(data)\n",
        "  \n",
        "  def validate_regression(self, data):\n",
        "    error = 0\n",
        "    for x, y in data:\n",
        "        error = error + np.square(self.feed_forward(x)-y).mean()\n",
        "    return error / len(data)\n",
        "\n",
        "  def classify(self, test_data):\n",
        "        results = []\n",
        "        for x in test_data:\n",
        "            classification = np.argmax(self.feed_forward(x))\n",
        "            results.append(classification)\n",
        "        return results\n",
        "  \n",
        "  def run_regression(self, test_data):\n",
        "        results = []\n",
        "        for x in test_data:\n",
        "            regression = self.feed_forward(x)\n",
        "            results.append(regression)\n",
        "        return results\n",
        "      \n",
        "  def take_snapshot(self, name, train_accuracy=None, test_accuracy=None):\n",
        "    if(self.using_bias):\n",
        "      dump = dict(biases=[bias.tolist() for bias in self.biases_weights],\n",
        "                  weights=[weight.tolist() for weight in self.edges_weights],\n",
        "                  name=name,\n",
        "                  train_fitness=train_accuracy,\n",
        "                  test_fitness=test_accuracy)\n",
        "    else:\n",
        "      dump = dict(weights=[weight.tolist() for weight in self.edges_weights],\n",
        "                  name=name,\n",
        "                  train_fitness=train_accuracy,\n",
        "                  test_fitness=test_accuracy)\n",
        "      \n",
        "    self.history.append(dump)\n",
        "\n",
        "  def save_history(self, out_filepath):\n",
        "    network_info = dict(\n",
        "        sizes=self.layer_sizes, learning_factor=self.learning_factor,\n",
        "        moment_factor=self.moment_factor,\n",
        "        epoch_num=self.epoch_num, batch_size=self.batch_size,\n",
        "        activation_function=self.activation_function.__name__,\n",
        "        output_function=self.output_function.__name__,\n",
        "        cost_function_deriv=self.cost_function_deriv.__name__,\n",
        "        seed=self.seed)\n",
        "    json.dump(dict(history=self.history, network_info=network_info),\n",
        "              codecs.open(out_filepath, 'w', encoding='utf-8'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3g1s3G3468Ga",
        "colab_type": "text"
      },
      "source": [
        "## Podpięcie dysku i wczytywanie danych"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9kob-tk7C5h",
        "colab_type": "text"
      },
      "source": [
        "### Podpięcie dysku"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlPVGpq77Ewk",
        "colab_type": "code",
        "outputId": "14c6eac2-1f9e-4898-8e25-425b41fa8891",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "BASE_DIR = '/content/gdrive/My Drive/DL2020/Projekt1a'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-naFIiYM7Gi8",
        "colab_type": "text"
      },
      "source": [
        "### Ładowanie danych"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awnuWVRX7Ih7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def load_classification_wrapper(path, name, train_size, test_size=None):\n",
        "    if test_size == None:\n",
        "        test_size = train_size\n",
        "\n",
        "    train = load_classification(os.path.join(\n",
        "        path, f'{name}.train.{train_size}.csv'), ['x', 'y'])\n",
        "    test = load_classification(os.path.join(\n",
        "        path, f'{name}.test.{test_size}.csv'), ['x', 'y'])\n",
        "    return (train, test)\n",
        "\n",
        "def mnist_load_classification(path):\n",
        "\n",
        "    df = pd.read_csv(os.path.join(path, 'train.csv'))\n",
        "\n",
        "    output_layer_neuron_count = 10\n",
        "    input_layer_neuron_count = 784\n",
        "\n",
        "    train = []\n",
        "    test = []\n",
        "\n",
        "    for ind in df.index:\n",
        "      x = np.zeros((input_layer_neuron_count, 1))\n",
        "      for i in range(0, 783):\n",
        "        x[i] = (df['pixel' + str(i)][ind] - 128) / 128\n",
        "      y = vectorize_class(df['label'][ind], output_layer_neuron_count)\n",
        "      if ind < 38000:\n",
        "        train.append((x, y))\n",
        "      else:\n",
        "        test.append((x, y))\n",
        "      if ind % 1000 == 0:\n",
        "        print('MNIST dataset loading: ' + str(ind) + '/42000 loaded.')\n",
        "\n",
        "    return (train, test)\n",
        "\n",
        "def mnist_load_classification_test(path):\n",
        "\n",
        "    df = pd.read_csv(os.path.join(path, 'test.csv'))\n",
        "\n",
        "    output_layer_neuron_count = 10\n",
        "    input_layer_neuron_count = 784\n",
        "\n",
        "    test = []\n",
        "\n",
        "    for ind in df.index:\n",
        "      x = np.zeros((input_layer_neuron_count, 1))\n",
        "      for i in range(0, 783):\n",
        "        x[i] = (df['pixel' + str(i)][ind] - 128) / 128\n",
        "      test.append(x)\n",
        "      if ind % 1000 == 0:\n",
        "        print('MNIST dataset loading: ' + str(ind) + '/28000 loaded.')\n",
        "\n",
        "    return test\n",
        "\n",
        "\n",
        "def load_classification(csv_filename, argument_column_names, class_column_name='cls'):\n",
        "    df = pd.read_csv(csv_filename)\n",
        "\n",
        "    output_layer_neuron_count = len(df[class_column_name].unique())\n",
        "    input_layer_neuron_count = len(argument_column_names)\n",
        "\n",
        "    training_data = []\n",
        "\n",
        "    for row in df.itertuples():\n",
        "        x = np.zeros((input_layer_neuron_count, 1))\n",
        "\n",
        "        for i in range(len(argument_column_names)):\n",
        "            x[i] = getattr(row, argument_column_names[i])\n",
        "\n",
        "        y = vectorize_class(getattr(row, class_column_name) - 1,\n",
        "                            output_layer_neuron_count)\n",
        "\n",
        "        training_data.append((x, y))\n",
        "    return training_data\n",
        "\n",
        "def load_regression(csv_filename, input_column_names, output_column_names):\n",
        "    df = pd.read_csv(csv_filename)\n",
        "\n",
        "    output_layer_neuron_count = len(output_column_names)\n",
        "    input_layer_neuron_count = len(input_column_names)\n",
        "\n",
        "    training_data = []\n",
        "\n",
        "    for row in df.itertuples():\n",
        "        x = np.zeros((input_layer_neuron_count, 1))\n",
        "        y = np.zeros((output_layer_neuron_count, 1))\n",
        "\n",
        "        for i in range(len(input_column_names)):\n",
        "            x[i] = getattr(row, input_column_names[i])\n",
        "\n",
        "        for i in range(len(output_column_names)):\n",
        "            y[i] = getattr(row, output_column_names[i])\n",
        "\n",
        "        training_data.append((x, y))\n",
        "    return training_data\n",
        "\n",
        "def load_regression_wrapper(path, name, train_size, test_size=None):\n",
        "    if test_size == None:\n",
        "        test_size = train_size\n",
        "\n",
        "    train = load_regression(os.path.join(\n",
        "        path, f'{name}.train.{train_size}.csv'), ['x'], ['y'])\n",
        "    test = load_regression(os.path.join(\n",
        "        path, f'{name}.test.{test_size}.csv'), ['x'], ['y'])\n",
        "    return (train, test)\n",
        "\n",
        "\n",
        "def vectorize_class(class_id, class_length):\n",
        "    y = np.zeros((class_length, 1))\n",
        "    y[class_id] = 1\n",
        "    return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfzy1TrtNKyv",
        "colab_type": "text"
      },
      "source": [
        "## Wczytywanie sieci z pliku"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9DgrGXBNOm9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import json \n",
        "\n",
        "def load_network_from_file(path):\n",
        "  with open(os.path.join('', path)) as f: \n",
        "    d = json.load(f)\n",
        "  \n",
        "  weights = d['history'][-1]['weights']\n",
        "  biases = d['history'][-1].get('biases', 'No bias')\n",
        "\n",
        "  switcher = {\n",
        "      \"sigmoid\": {\n",
        "          \"function\": sigmoid,\n",
        "          \"derivative\": sigmoid_derivative\n",
        "      },\n",
        "      \"relu\": {\n",
        "          \"function\": relu,\n",
        "          \"derivative\": relu_derivative\n",
        "      },\n",
        "      \"identity\": {\n",
        "          \"function\": identity,\n",
        "          \"derivative\": identity_derivative\n",
        "      },\n",
        "      \"tanh\": {\n",
        "          \"function\": tanh,\n",
        "          \"derivative\": tanh_derivative\n",
        "      }\n",
        "  }\n",
        "\n",
        "  activation_function = switcher.get(d['network_info']['activation_function'], \"Invalid argument\")\n",
        "  output_function = switcher.get(d['network_info']['output_function'], \"Invalid argument\")\n",
        "\n",
        "  network = NeuralNetwork([], activation_function['function'], activation_function['derivative'], output_function['function'], output_function['derivative'], quadratic_cost_derivative, False, TaskType.Classification)\n",
        "  network.edges_weights = (weights)\n",
        "  if biases != 'No bias':\n",
        "    network.using_bias = True\n",
        "    network.biases_weights = biases\n",
        "  else:\n",
        "    network.using_bias = False\n",
        "\n",
        "  return network\n",
        "\n",
        "nn = load_network_from_file(BASE_DIR + r'/results/mnist_2020-03-20 18:02:25.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80unqXUN7-vW",
        "colab_type": "text"
      },
      "source": [
        "## Uruchomienie sieci"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vq0gcLoPrT0q",
        "colab_type": "text"
      },
      "source": [
        "### Klasyfikacja"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnNerGan8BYI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "train, test = load_classification_wrapper(BASE_DIR + r'/data/classification', 'data.three_gauss', 100)\n",
        "inp_lay_size = len(train[0][0])\n",
        "out_lay_size = len(train[0][1])\n",
        "np.random.seed(11)\n",
        "nn = NeuralNetwork([inp_lay_size, 8, out_lay_size], sigmoid, sigmoid_derivative, sigmoid, sigmoid_derivative, quadratic_cost_derivative, True, TaskType.Classification)\n",
        "nn.learn(train, test, 0.9, 0.3, 20, 500)\n",
        "\n",
        "now = datetime.now()\n",
        "current_time = now.strftime(\"%H:%M:%S\")\n",
        "\n",
        "nn.save_history(BASE_DIR + f'/results/classification_{current_time}.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhSifZGurZo_",
        "colab_type": "text"
      },
      "source": [
        "### Regresja"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gF4_TE8YxtMW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "train, test = load_regression_wrapper(BASE_DIR + r'/data/regression', 'data.activation', 100)\n",
        "inp_lay_size = len(train[0][0])\n",
        "out_lay_size = len(train[0][1])\n",
        "np.random.seed(0)\n",
        "nn = NeuralNetwork([inp_lay_size, 5, out_lay_size], sigmoid, sigmoid_derivative, identity, identity_derivative, quadratic_cost_derivative, True, TaskType.Regression)\n",
        "nn.learn(train, test, 0.1, 0, 20, 100)\n",
        "\n",
        "now = datetime.now()\n",
        "current_time = now.strftime(\"%H:%M:%S\")\n",
        "\n",
        "nn.save_history(BASE_DIR + f'/results/regression_{current_time}.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOmG42C3c3QK",
        "colab_type": "text"
      },
      "source": [
        "## Testy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zISQ5_Myc6pJ",
        "colab_type": "text"
      },
      "source": [
        "### Funkcja feed forward"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE5bhR9PdANF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(0)\n",
        "l_sizes = [2,2,2]\n",
        "w = [np.random.randn(x, y) * 0.01 for x, y in zip(l_sizes[1:], l_sizes[:-1])]\n",
        "b = [np.random.rand(x, 1) for x in l_sizes[1:]]\n",
        "\n",
        "inp = np.array([[1], [1]])\n",
        "res2 = inp\n",
        "for ww, bb in zip(w, b):\n",
        "  res2 = identity(np.dot(ww, res2) + bb)\n",
        "\n",
        "net = NeuralNetwork([2, 2, 2], identity, identity_derivative, identity, identity_derivative, quadratic_cost_derivative, True, TaskType.Classification, 0)\n",
        "res = net.feed_forward(inp)\n",
        "print(res)\n",
        "print(res2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jG41OXvmV7Q",
        "colab_type": "text"
      },
      "source": [
        "### Wsteczna propagacja, gdy błąd jest zerowy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyRgIUphmUC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(0)\n",
        "l_sizes = [2,2,2]\n",
        "output = np.array([[0.80606424], [0.53762709]])\n",
        "inp = np.array([[1], [1]])\n",
        "\n",
        "net = NeuralNetwork([2, 2, 2], identity, identity_derivative, identity, identity_derivative, quadratic_cost_derivative, True, TaskType.Classification, 0)\n",
        "w, b = net.backpropagate(inp, output)\n",
        "print(w)\n",
        "print(b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5AAfuVYwtc2",
        "colab_type": "text"
      },
      "source": [
        "### Uczenie sieci przy zerowym błędzie na starcie"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Krur2o9Yw3xF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(0)\n",
        "l_sizes = [2,2,2]\n",
        "w = [np.random.randn(x, y) * 0.01 for x, y in zip(l_sizes[1:], l_sizes[:-1])]\n",
        "b = [np.random.rand(x, 1) for x in l_sizes[1:]]\n",
        "\n",
        "training_set = [(np.array([[1], [1]]), np.array([[0.80606424], [0.53762709]]))]\n",
        "test_set = [(np.array([[1], [1]]), np.array([[0.80606424], [0.53762709]]))]\n",
        "\n",
        "net = NeuralNetwork([2, 2, 2], identity, identity_derivative, identity, identity_derivative, quadratic_cost_derivative, True, TaskType.Classification, 0)\n",
        "net.learn(training_set, test_set, 0.1, 0.1, 1, 1)\n",
        "print(net.feed_forward(np.array([[1], [1]])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZVyAWbtbwTK",
        "colab_type": "text"
      },
      "source": [
        "## Wyniki"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MS7AKVOFeMBX",
        "colab_type": "text"
      },
      "source": [
        "###Funkcje pomocnicze"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9W2Ygco_eRnC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_filename(figure_name, data_name, epochs, batch_size, task_type):\n",
        "    return BASE_DIR + f'/results/{task_type}/{figure_name}{task_type}__{data_name.split(\".\")[1]}__{epochs}_{batch_size}.png'\n",
        "\n",
        "def create_heatmap(min_x, max_x, min_y, max_y, count_x, count_y):\n",
        "    step_x = (max_x - min_x) / count_x\n",
        "    step_y = (max_y - min_y) / count_y\n",
        "    probes_x = []\n",
        "    probes_y = []\n",
        "    probes_input = []\n",
        "    for x_i in range(count_x):\n",
        "        for y_i in range(count_y):\n",
        "            x = min_x + x_i * step_x\n",
        "            y = min_y + y_i * step_y\n",
        "            probes_x.append(x)\n",
        "            probes_y.append(y)\n",
        "            input_x = np.zeros((2, 1))\n",
        "            input_x[0] = x\n",
        "            input_x[1] = y\n",
        "            probes_input.append(input_x)\n",
        "    return probes_x, probes_y, probes_input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AK8cQC16b7pV",
        "colab_type": "text"
      },
      "source": [
        "### Wyniki klasyfikacji"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alc9vAD0cW6B",
        "colab_type": "text"
      },
      "source": [
        "#### Wizualizacja zbioru uczącego oraz rezultatów klasyfikacji"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jj6pFqrBclBY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plot\n",
        "\n",
        "def visualize_classification(seed, hidden_layer_sizes, hidden_layers_activation_function, hidden_layers_activation_function_prime, output_layer_activation_function, output_layer_activation_function_prime, cost_derivative, is_bias_enabled, path, data_name, train_size,  epochs, batch_size, eta, alpha, test_size=None):\n",
        "    global nn\n",
        "    train_data, test_data = load_classification_wrapper(path, data_name, train_size, test_size)\n",
        "    np.random.seed(seed)\n",
        "    sizes = [len(train_data[0][0])]\n",
        "    sizes.extend(hidden_layer_sizes)\n",
        "    sizes.append(len(train_data[0][1]))\n",
        "    nn = NeuralNetwork(sizes, hidden_layers_activation_function, hidden_layers_activation_function_prime, output_layer_activation_function, output_layer_activation_function_prime, cost_derivative, is_bias_enabled, TaskType.Classification)\n",
        "\n",
        "    x = [entry[0][0][0] for entry in test_data]\n",
        "    y = [entry[0][1][0] for entry in test_data]\n",
        "\n",
        "    probes_x, probes_y, probes_input = create_heatmap(\n",
        "        min(x), max(x), min(y), max(y), len(x), len(y))\n",
        "\n",
        "    nn.learn(train_data, test_data, eta, alpha, batch_size, epochs, probe=probes_input)\n",
        "    \n",
        "    # classification points visualization\n",
        "    valid_classification = [np.argmax(entry[1]) for entry in test_data]\n",
        "    net_classification = nn.classify([x for (x, y) in test_data])\n",
        "    x = [entry[0][0][0] for entry in test_data]\n",
        "    y = [entry[0][1][0] for entry in test_data]\n",
        "\n",
        "    plot.scatter(x=x, y=y, c=net_classification)\n",
        "    plot.axis('equal')\n",
        "    plot.title(f'classification results for {data_name} size {train_size}')\n",
        "    plot.xlabel('x')\n",
        "    plot.ylabel('y')\n",
        "    plot.savefig(create_filename('results-', data_name, epochs, batch_size, 'classification'), dpi=150)\n",
        "    plot.show()\n",
        "    plot.clf()\n",
        "\n",
        "    # classificaiton area probing\n",
        "\n",
        "    for probe_classification, epoch in zip(nn.classification_area_probing, range(len(nn.classification_area_probing))):\n",
        "        plot.scatter(x=probes_x, y=probes_y, c=probe_classification)\n",
        "        plot.axis('equal')\n",
        "        plot.title(f'probe classification for {data_name} size {train_size} epoch {epoch}')\n",
        "        plot.xlabel('x')\n",
        "        plot.ylabel('y')\n",
        "        plot.savefig(create_filename(f'learning-{epoch}-', data_name, epochs, batch_size, 'classification'), dpi=150)\n",
        "        plot.show()\n",
        "        plot.clf()\n",
        "\n",
        "visualize_classification(0, [5], sigmoid, sigmoid_derivative, sigmoid, sigmoid_derivative, quadratic_cost_derivative, True, BASE_DIR + r'/data/classification', 'data.three_gauss', 100, 10, 10, 0.9, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVbp7NS-W1L_",
        "colab_type": "text"
      },
      "source": [
        "#### Wykresy błędu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dyA9tP7W9Nx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_error(seed, hidden_layer_sizes, hidden_layers_activation_function, hidden_layers_activation_function_prime, output_layer_activation_function, output_layer_activation_function_prime, cost_derivative, is_bias_enabled, path, data_name, train_size,  epochs, batch_size, eta, alpha, test_size=None):\n",
        "  train_data, test_data = load_classification_wrapper(path, data_name, train_size, test_size)\n",
        "  np.random.seed(seed)\n",
        "  sizes = [len(train_data[0][0])]\n",
        "  sizes.extend(hidden_layer_sizes)\n",
        "  sizes.append(len(train_data[0][1]))\n",
        "  nn = NeuralNetwork(sizes, hidden_layers_activation_function, hidden_layers_activation_function_prime, output_layer_activation_function, output_layer_activation_function_prime, cost_derivative, is_bias_enabled, TaskType.Classification, seed)\n",
        "  nn.learn(train_data, test_data, eta, alpha, batch_size, epochs)\n",
        "  train_acc = []\n",
        "  test_acc = []\n",
        "  for dump in nn.history:\n",
        "    train_acc.append(1 - dump[\"train_fitness\"])\n",
        "    test_acc.append(1 - dump[\"test_fitness\"])\n",
        "    \n",
        "  plt.plot(range(nn.epoch_num), train_acc, 'r', label = 'Train data')\n",
        "  plt.plot(range(nn.epoch_num), test_acc, 'g', label = 'Test data')\n",
        "  plt.xlabel(\"Epoch number\")\n",
        "  plt.ylabel(\"Error\")\n",
        "  plt.title(f\"Classification error for {data_name} size {train_size} \\n Layer count: {len(sizes)}, Hidden layer sizes: {hidden_layer_sizes}, \\n\"  + \\\n",
        "            f\"Hidden layer function: {hidden_layers_activation_function.__name__}, \\n Output layer function: {output_layer_activation_function.__name__}, \\n Bias: {is_bias_enabled}, \" + \\\n",
        "            f\"Learning rate: {eta}, \\n Moment rate: {alpha}\")\n",
        "  plt.legend(framealpha=1, frameon=True);\n",
        "  plt.show()\n",
        "\n",
        "visualize_error(0, [], sigmoid, sigmoid_derivative, sigmoid, sigmoid_derivative, cross_entropy_cost_derivative, True, BASE_DIR + r'/data/classification', 'data.simple', 100, 100, 10, 1, 0)\n",
        "visualize_error(0, [2], sigmoid, sigmoid_derivative, sigmoid, sigmoid_derivative, cross_entropy_cost_derivative, True, BASE_DIR + r'/data/classification', 'data.simple', 100, 100, 10, 1, 0)\n",
        "visualize_error(0, [3, 2], sigmoid, sigmoid_derivative, sigmoid, sigmoid_derivative, cross_entropy_cost_derivative, True, BASE_DIR + r'/data/classification', 'data.simple', 100, 100, 10, 1, 0)\n",
        "visualize_error(0, [2, 5, 2], sigmoid, sigmoid_derivative, sigmoid, sigmoid_derivative, cross_entropy_cost_derivative, True, BASE_DIR + r'/data/classification', 'data.simple', 100, 100, 10, 1, 0)\n",
        "visualize_error(0, [2, 2, 2], sigmoid, sigmoid_derivative, sigmoid, sigmoid_derivative, cross_entropy_cost_derivative, True, BASE_DIR + r'/data/classification', 'data.simple', 100, 100, 10, 1, 0)\n",
        "visualize_error(0, [2, 5, 5, 2], sigmoid, sigmoid_derivative, sigmoid, sigmoid_derivative, cross_entropy_cost_derivative, True, BASE_DIR + r'/data/classification', 'data.simple', 100, 100, 10, 1, 0)\n",
        "visualize_error(0, [2, 2, 2 ,2], sigmoid, sigmoid_derivative, sigmoid, sigmoid_derivative, cross_entropy_cost_derivative, True, BASE_DIR + r'/data/classification', 'data.simple', 100, 100, 10, 1, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Smbye5MZdjFl",
        "colab_type": "text"
      },
      "source": [
        "###Wyniki regresji"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPrqGk1Fdphs",
        "colab_type": "text"
      },
      "source": [
        "Wizualizacja zbioru uczącego oraz rezultatów regresji"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7og9HqUds8L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plot\n",
        "\n",
        "def visualize_regression(seed, hidden_layer_sizes, hidden_layers_activation_function, hidden_layers_activation_function_prime, output_layer_activation_function, output_layer_activation_function_prime, cost_derivative, is_bias_enabled, path, data_name, train_size,  epochs, batch_size, eta, alpha, test_size=None):\n",
        "    global nn\n",
        "    train_data, test_data = load_regression_wrapper(path, data_name, train_size, test_size)\n",
        "    np.random.seed(seed)\n",
        "    sizes = [len(train_data[0][0])]\n",
        "    sizes.extend(hidden_layer_sizes)\n",
        "    sizes.append(len(train_data[0][1]))\n",
        "    nn = NeuralNetwork(sizes, hidden_layers_activation_function, hidden_layers_activation_function_prime, output_layer_activation_function, output_layer_activation_function_prime, cost_derivative, is_bias_enabled, TaskType.Regression)\n",
        "\n",
        "    nn.learn(train_data, test_data, eta, alpha, batch_size, epochs)\n",
        "    \n",
        "    # regression points visualization\n",
        "    y_pred = nn.run_regression([x for (x, y) in test_data])\n",
        "    x = [entry[0][0][0] for entry in test_data]\n",
        "    y_true = [y for (x, y) in test_data]\n",
        "\n",
        "    min_x = min(x)\n",
        "    max_x = max(x)\n",
        "\n",
        "    plot.scatter(x=x, y=y_true, c='b', label='y_true')\n",
        "    plot.scatter(x=x, y=y_pred, c='r', label='y_pred')\n",
        "    plot.xlim(min_x, max_x)\n",
        "    plot.title(f'regression results for {data_name} size {train_size}')\n",
        "    plot.xlabel('x')\n",
        "    plot.ylabel('y')\n",
        "    plot.legend(loc='upper left')\n",
        "    plot.savefig(create_filename('results-', data_name, epochs, batch_size, 'regression'), dpi=150)\n",
        "    plot.show()\n",
        "    plot.clf()\n",
        "\n",
        "visualize_regression(0, [5], sigmoid, sigmoid_derivative, identity, identity_derivative, quadratic_cost_derivative, True, BASE_DIR + r'/data/regression', 'data.activation', 100, 100, 20, 0.1, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBrRA4TYjrLY",
        "colab_type": "text"
      },
      "source": [
        "#### Wykresy błędu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Qsbvi52jtiZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_error(seed, hidden_layer_sizes, hidden_layers_activation_function, hidden_layers_activation_function_prime, output_layer_activation_function, output_layer_activation_function_prime, cost_derivative, is_bias_enabled, path, data_name, train_size,  epochs, batch_size, eta, alpha, test_size=None):\n",
        "  train_data, test_data = load_regression_wrapper(path, data_name, train_size, test_size)\n",
        "  np.random.seed(seed)\n",
        "  sizes = [len(train_data[0][0])]\n",
        "  sizes.extend(hidden_layer_sizes)\n",
        "  sizes.append(len(train_data[0][1]))\n",
        "  nn = NeuralNetwork(sizes, hidden_layers_activation_function, hidden_layers_activation_function_prime, output_layer_activation_function, output_layer_activation_function_prime, cost_derivative, is_bias_enabled, TaskType.Regression, seed)\n",
        "  nn.learn(train_data, test_data, eta, alpha, batch_size, epochs)\n",
        "  train_err = []\n",
        "  test_err = []\n",
        "  for dump in nn.history:\n",
        "    train_err.append(dump[\"train_fitness\"])\n",
        "    test_err.append(dump[\"test_fitness\"])\n",
        "  plt.plot(range(nn.epoch_num), train_err, 'r', label='Train data')\n",
        "  plt.plot(range(nn.epoch_num), test_err, 'g', label='Test data')\n",
        "  plt.xlabel(\"Epoch number\")\n",
        "  plt.ylabel(\"Error\")\n",
        "  plt.title(f\"Regression error for {data_name} size {train_size} \\n Layer count: {len(sizes)}, Hidden layer sizes: {hidden_layer_sizes}, \\n\"  + \\\n",
        "            f\"Hidden layer function: {hidden_layers_activation_function.__name__}, \\n Output layer function: {output_layer_activation_function.__name__}, \\n Bias: {is_bias_enabled}, \" + \\\n",
        "            f\"Learning rate: {eta}, \\n Moment rate: {alpha}\")\n",
        "  plt.legend(framealpha=1, frameon=True);\n",
        "  plt.show()\n",
        "\n",
        "visualize_error(0, [], sigmoid, sigmoid_derivative, identity, identity_derivative, quadratic_cost_derivative, True, BASE_DIR + r'/data/regression', 'data.activation', 100, 100, 10, 0.01, 0.25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ob8CW1bvTsCy",
        "colab_type": "text"
      },
      "source": [
        "## MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-Ub0HQDJDrR",
        "colab_type": "text"
      },
      "source": [
        "### Ładowanie danych"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRXSL4xOTvlv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, test = mnist_load_classification(BASE_DIR + r'/data/MNIST')\n",
        "inp_lay_size = len(train[0][0])\n",
        "out_lay_size = len(train[0][1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAqB1bgMsroz",
        "colab_type": "text"
      },
      "source": [
        "### Liczności klas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_umT5wYnswa8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame(data=test, columns=['x','y'])\n",
        "ys = []\n",
        "for x in df['y']:\n",
        "  number = np.argmax(x)\n",
        "  ys.append(number)\n",
        "df['y'] = ys\n",
        "df.groupby('y').count()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSt_3feNJH56",
        "colab_type": "text"
      },
      "source": [
        "### Uczenie sieci"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbV7QmnEJKXe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eta = 1\n",
        "alpha = 0.25\n",
        "epochs = 100\n",
        "batch_size = 20\n",
        "hidden_layer_sizes = [64]\n",
        "seed = 5\n",
        "\n",
        "np.random.seed(seed)\n",
        "\n",
        "sizes = [inp_lay_size]\n",
        "sizes.extend(hidden_layer_sizes)\n",
        "sizes.append(out_lay_size)\n",
        "\n",
        "nn = NeuralNetwork(sizes, sigmoid, sigmoid_derivative, sigmoid, sigmoid_derivative, quadratic_cost_derivative, True, TaskType.Classification)\n",
        "nn.learn(train, test, eta, alpha, batch_size, epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HU916X-CeKbl",
        "colab_type": "text"
      },
      "source": [
        "### Uczenie sieci - wiele prób"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaDoDj6TeT38",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "number_of_trials = 10\n",
        "eta = 1\n",
        "alpha = 0.25\n",
        "epochs = 100\n",
        "batch_size = 10\n",
        "hidden_layer_sizes = [32]\n",
        "hidden_func = sigmoid\n",
        "hidden_func_deriv = sigmoid_derivative\n",
        "output_func = sigmoid\n",
        "output_func_deriv = sigmoid_derivative\n",
        "cost_func_deriv = quadratic_cost_derivative\n",
        "is_bias = True\n",
        "save_log = True\n",
        "\n",
        "sizes = [inp_lay_size]\n",
        "sizes.extend(hidden_layer_sizes)\n",
        "sizes.append(out_lay_size)\n",
        "\n",
        "for i in range(number_of_trials):\n",
        "  nn = NeuralNetwork(sizes, hidden_func, hidden_func_deriv, output_func, output_func_deriv, cost_func_deriv, is_bias, TaskType.Classification, i)\n",
        "  nn.learn(train, test, eta, alpha, batch_size, epochs)\n",
        "  print(f'Best accuracy in iteration {i}: {nn.best_acc} - Epoch {nn.best_epoch}')\n",
        "  if(save_log):\n",
        "    now = datetime.now()\n",
        "    current_time = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    nn.save_history(BASE_DIR + f'/results/mnist_{current_time}.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fSsCEShJodp",
        "colab_type": "text"
      },
      "source": [
        "### Wykresy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWQNGB_JJeNJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "for dump in nn.history:\n",
        "  train_acc.append(1 - dump[\"train_fitness\"])\n",
        "  test_acc.append(1 - dump[\"test_fitness\"])\n",
        "  \n",
        "plt.plot(range(nn.epoch_num), train_acc, 'r', label = 'Train data')\n",
        "plt.plot(range(nn.epoch_num), test_acc, 'g', label = 'Test data')\n",
        "plt.xlabel(\"Epoch number\")\n",
        "plt.ylabel(\"Error\")\n",
        "plt.title(f\"Classification error for MNIST \\n Layer count: {len(sizes)}, Hidden layer sizes: {hidden_layer_sizes}, \\n\"  + \\\n",
        "          f\"Epochs: {epochs}, Batch size: {batch_size}, \\n\" + \\\n",
        "          f\"Learning rate: {eta}, Moment rate: {alpha}\")\n",
        "plt.legend(framealpha=1, frameon=True);\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoxqZmQDSluA",
        "colab_type": "text"
      },
      "source": [
        "### Wykonanie klasyfikacji na zbiorze testowym"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqrpvb6ZSqDJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dataset = mnist_load_classification_test(BASE_DIR + r'/data/MNIST')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v81SoL_rT7Qh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net_classification = nn.classify([x for x in test_dataset])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsPlZmW5U6PX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datetime import datetime\n",
        "from numpy import savetxt\n",
        "\n",
        "results = []\n",
        "\n",
        "it = 1\n",
        "\n",
        "for x in net_classification:\n",
        "  results.append((it, x))\n",
        "  it = it + 1\n",
        "\n",
        "now = datetime.now()\n",
        "current_time = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "savetxt(BASE_DIR + f'/results/results_{current_time}.csv', results, delimiter=',', fmt='%i')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}